@article{cdc_disability_2018,
	title = {Prevalence of Disabilities and Health Care Access by Disability Status and Type Among Adults — United States, 2016},
	doi = {10.15585/mmwr.mm6732a3},
	language = {en-us},
	journal = {Centers for Disease Control and Prevention MMWR},
	author = {Okoro, CA and Hollis, ND and Cyrus, AC and Griffin-Blake, S.},
	year = {2018},
	volume={67},
    pages={882–887}
}

@ARTICLE{9023497,
  author={Lee, Bongshin and Choe, Eun Kyoung and Isenberg, Petra and Marriott, Kim and Stasko, John},
  journal={IEEE Computer Graphics and Applications}, 
  title={Reaching Broader Audiences With Data Visualization}, 
  year={2020},
  volume={40},
  number={2},
  pages={82-90},
  doi={10.1109/MCG.2020.2968244}
}

@techreport{south_detecting_2021,
	title = {Detecting and {Defending} {Against} {Seizure}-{Inducing} {GIFs} in {Social} {Media}},
	abstract = {Despite recent improvements in online accessibility, the Internet remains an inhospitable place for users with photosensitive epilepsy, a chronic condition in which certain light stimuli can trigger seizures and even lead to death in extreme cases. In this paper, we explore how current risk detection systems have allowed attackers to take advantage of design oversights and target vulnerable users with photosensitivity on popular social media platforms. Through interviews with photosensitive individuals and a critical review of existing systems, we constructed design requirements for consumer-driven protective systems and developed a prototype browser ex-tension for actively detecting and disarming potentially seizure-
inducing GIFs and videos. We validate our system with a comprehensive dataset of simulated GIFs and GIFs collected from social media. Finally, we conduct a novel quantitative analysis of the prevalence of seizure-inducing GIFs across popular social media
platforms and contribute recommendations for improving online accessibility for individuals with photosensitivity. All study materials
are available at https://osf.io/5a3dy/.},
	note = {Accessed: 2021-08-20},
	institution = {OSF Preprints},
	author = {South, Laura and Saffo, David and Borkin, Michelle},
	month = jan,
	year = {2021},
	doi = {10.31219/osf.io/4kgu6},
	keywords = {chartability, Accessibility, Computer Sciences, GIFs, Graphics and Human Computer Interfaces, Human-computer interaction, peat, Photosensitive epilepsy, Physical Sciences and Mathematics, seizure, Social media, visualization},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/85UGZ8SM/South et al. - 2021 - Detecting and Defending Against Seizure-Inducing G.pdf:application/pdf},
}

@misc{noauthor_axe-core_2021,
	title = {axe-core},
	author = {Deque},
	copyright = {MPL-2.0},
	url = {https://github.com/dequelabs/axe-core},
	abstract = {Accessibility engine for automated Web UI testing},
	note = {Accessed: 2021-08-20},
	publisher = {Deque Systems Inc.},
	month = aug,
	year = {2021},
	keywords = {chartability, a11y, accessibility, axe, axe-core, deque, hacktoberfest},
}

@misc{noauthor_fixed_nodate,
	title = {Fixed placement columns {\textbar} {Highcharts}.com},
	author = {Highsoft},
	url = {https://www.highcharts.com/demo/column-placement},
	note = {Accessed: 2021-08-20},
	keywords = {chartability, accessibility, audit, highcharts},
	file = {Fixed placement columns | Highcharts.com:/Users/Elavsky/Zotero/storage/R3KM8CIS/column-placement.html:text/html},
}

@misc{noauthor_highcharts_nodate,
	title = {Highcharts},
	author = {Highsoft},
	url = {https://www.npmjs.com/package/highcharts},
	abstract = {JavaScript charting framework},
	language = {en},
	note = {Accessed: 2021-08-20},
	journal = {npm},
	keywords = {chartability, accessibility, highcharts, npm},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/BW6EAV7E/highcharts.html:text/html},
}

@misc{noauthor_naked_nodate,
	title = {The {Naked} {Truth}},
	author = {Amaka, Ofunne and Thomas, Amber},
	url = {https://pudding.cool/2021/03/foundation-names},
	abstract = {How the names of 6,816 complexion products can reveal bias in beauty},
	language = {en},
	note = {Accessed: 2021-08-20},
	journal = {The Pudding},
	keywords = {chartability, amber thomas, pudding},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/YKIZCYFN/foundation-names.html:text/html},
}

@misc{noauthor_we_nodate,
	title = {Coin Flip Game.},
	author = {DeMartini, Chris},
	url = {https://public.tableau.com/views/CoinFlipGame/CoinFlipGame},
	language = {en},
	note = {Accessed: 2021-08-20},
	journal = {Tableau Software},
	keywords = {chartability, demartini, tableau},
}

@misc{initiative_wai_web_2021,
	title = {Web {Accessibility} {Laws} \& {Policies}},
	author = {Web Accessibility Initiative},
	url = {https://www.w3.org/WAI/policies/},
	abstract = {Dated},
	language = {en},
	note = {Accessed: 2021-08-19},
	journal = {Web Accessibility Initiative (WAI)},
	month = aug,
	year = {2021},
	keywords = {chartability, policy, wcag},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/P3HLXPUR/policies.html:text/html},
}

@inproceedings{chuan_usability_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Usability {Heuristics} for {Heuristic} {Evaluation} of {Gestural} {Interaction} in {HCI}},
	isbn = {978-3-319-20886-2},
	doi = {10.1007/978-3-319-20886-2_14},
	abstract = {Heuristic evaluation, also known as discounted usability engineering method, is a quick and very effective form of usability testing performed typically by usability experts or domain experts. However, in the field of gestural interaction testing, general-purpose usability heuristic framework may not be sufficient to evaluate the usability validity of gestures used. Gestural interaction could be found in products from mainstream touchscreen devices to emerging technologies such as motion tracking, augmented virtual reality, and holograms. Usability testing by experts during the early stages of product development that utilizes emerging technologies of gestural interaction is desirable. Therefore, this study has the objective to create a set of gesture heuristics that can be used in conjunction and with minimal conflict with existing general-purpose usability heuristics for the purpose of designing and testing new gestural interaction. In order to do so, this study reviews literature of gestural interaction and usability testing to find and evaluate previous gesture heuristics. The result is a condensed set of four gesture-specific heuristics comprising Learnability, Cognitive Workload, Adaptability and Ergonomics. Paired sample t-test analysis revealed that significantly more defects were discovered when gesture heuristics knowledge were used for evaluation of gestural interaction.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Design} {Discourse}},
	publisher = {Springer International Publishing},
	author = {Chuan, Ngip Khean and Sivaji, Ashok and Ahmad, Wan Fatimah Wan},
	editor = {Marcus, Aaron},
	year = {2015},
	keywords = {chartability, Gestural interaction, hci, heuristic, Heuristic evaluation, Interaction styles, usability, Usability testing, User experience},
	pages = {138--148},
	file = {Springer Full Text PDF:/Users/Elavsky/Zotero/storage/2W7SQAH9/Chuan et al. - 2015 - Usability Heuristics for Heuristic Evaluation of G.pdf:application/pdf},
}

@inproceedings{macdonald_changing_2013,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '13},
	title = {Changing perspectives on evaluation in {HCI}: past, present, and future},
	isbn = {978-1-4503-1952-2},
	shorttitle = {Changing perspectives on evaluation in {HCI}},
	doi = {10.1145/2468356.2468714},
	abstract = {Evaluation has been a dominant theme in HCI for decades, but it is far from being a solved problem. As interactive systems and their uses change, the nature of evaluation must change as well. In this paper, we outline the challenges our community needs to address to develop adequate methods for evaluating systems in modern (and future) use contexts. We begin by tracing how evaluation efforts have been shaped by a continuous adaptation to technological and cultural changes and conclude by discussing important research directions that will shape evaluation's future.},
	note = {Accessed: 2021-08-19},
	booktitle = {{CHI} '13 {Extended} {Abstracts} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {MacDonald, Craig M. and Atwood, Michael E.},
	month = apr,
	year = {2013},
	keywords = {chartability, hci, usability, evaluation, history, user experience},
	pages = {1969--1978},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/MB5W8GYW/MacDonald and Atwood - 2013 - Changing perspectives on evaluation in HCI past, .pdf:application/pdf},
}

@incollection{oulasvirta_hci_2016,
	address = {New York, NY, USA},
	title = {{HCI} {Research} as {Problem}-{Solving}},
	isbn = {978-1-4503-3362-7},
	doi = {10.1145/2858036.2858283},
	abstract = {This essay contributes a meta-scientific account of human-computer interaction (HCI) research as problem-solving. We build on the philosophy of Larry Laudan, who develops problem and solution as the foundational concepts of science. We argue that most HCI research is about three main types of problem: empirical, conceptual, and constructive. We elaborate upon Laudan's concept of problem-solving capacity as a universal criterion for determining the progress of solutions (outcomes): Instead of asking whether research is 'valid' or follows the 'right' approach, it urges us to ask how its solutions advance our capacity to solve important problems in human use of computers. This offers a rich, generative, and 'discipline-free' view of HCI and resolves some existing debates about what HCI is or should be. It may also help unify efforts across nominally disparate traditions in empirical research, theory, design, and engineering.},
	note = {Accessed: 2021-08-19},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Oulasvirta, Antti and Hornbæk, Kasper},
	month = may,
	year = {2016},
	keywords = {chartability, hci, constructive research, human-computer interaction, Larry Laudan, problem-solving, research problem, scientific progress},
	pages = {4956--4967},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/9GDMFMJ8/Oulasvirta and Hornbæk - 2016 - HCI Research as Problem-Solving.pdf:application/pdf},
}

@incollection{ledo_evaluation_2018,
	address = {New York, NY, USA},
	title = {Evaluation {Strategies} for {HCI} {Toolkit} {Research}},
	isbn = {978-1-4503-5620-6},
	doi = {10.1145/3173574.3173610},
	abstract = {Toolkit research plays an important role in the field of HCI, as it can heavily influence both the design and implementation of interactive systems. For publication, the HCI community typically expects toolkit research to include an evaluation component. The problem is that toolkit evaluation is challenging, as it is often unclear what 'evaluating' a toolkit means and what methods are appropriate. To address this problem, we analyzed 68 published toolkit papers. From our analysis, we provide an overview of, reflection on, and discussion of evaluation methods for toolkit contributions. We identify and discuss the value of four toolkit evaluation strategies, including the associated techniques that each employs. We offer a categorization of evaluation strategies for toolkit researchers, along with a discussion of the value, potential limitations, and trade-offs associated with each strategy.},
	note = {Accessed: 2021-08-19},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ledo, David and Houben, Steven and Vermeulen, Jo and Marquardt, Nicolai and Oehlberg, Lora and Greenberg, Saul},
	month = apr,
	year = {2018},
	keywords = {chartability, hci, evaluation, design, prototyping, toolkit, toolkits, user interfaces},
	pages = {1--17},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/7RS3S8ZD/Ledo et al. - 2018 - Evaluation Strategies for HCI Toolkit Research.pdf:application/pdf},
}

@inproceedings{sharif_understanding_2021,
	author = {Sharif, Ather and Chintalapati, Sanjana Shivani and Wobbrock, Jacob O. and Reinecke, Katharina},
    title = {Understanding Screen-Reader Users’ Experiences with Online Data Visualizations},
    year = {2021},
    isbn = {9781450383066},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3441852.3471202},
    doi = {10.1145/3441852.3471202},
    booktitle = {The 23rd International ACM SIGACCESS Conference on Computers and Accessibility},
    articleno = {14},
    numpages = {16},
    keywords = {data, challenges, techniques, screen readers, visualizations},
    location = {Virtual Event, USA},
    series = {ASSETS '21}
}

@misc{noauthor_webaim_nodate,
	title = {{WebAIM}: {The} {WebAIM} {Million} - {An} annual accessibility analysis of the top 1,000,000 home pages},
	author = {WebAIM},
	url = {https://webaim.org/projects/million/\#wcag},
	note = {Accessed: 2021-09-03},
	keywords = {chartability, accessibility, analysis, web, webaim million},
	file = {WebAIM\: The WebAIM Million - An annual accessibility analysis of the top 1,000,000 home pages:/Users/Elavsky/Zotero/storage/JMXANEWM/million.html:text/html},
}

@misc{noauthor_build_nodate,
	title = {Build {Dashboards} for {Accessibility}},
	author = {Tableau},
	url = {https://help.tableau.com/current/pro/desktop/en-us/accessibility_dashboards.htm},
	abstract = {If you want to make your dashboards accessible to as many people as possible, or if you work in an environment that is subject to US Section 508 requirements, other accessibility-related laws and regulations, you can use Tableau to create dashboards that conform to the Web Content Accessibility guidelines (WCAG 2},
	language = {en-us},
	note = {Accessed: 2021-09-03},
	keywords = {chartability, accessibility, tableau, keyboard navigation},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/PAIXJQWM/accessibility_dashboards.html:text/html},
}

@misc{noauthor_mapbox-gl_nodate,
	title = {mapbox-gl js},
	author = {Mapbox},
	url = {https://www.npmjs.com/package/mapbox-gl},
	abstract = {A WebGL interactive maps library},
	language = {en},
	note = {Accessed: 2021-09-03},
	journal = {npm},
	keywords = {chartability, npm, mapbox gl},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/4NM2JV3V/mapbox-gl.html:text/html},
}

@misc{noauthor_world_nodate,
	title = {World report on vision},
	url = {https://www.who.int/publications-detail-redirect/9789241516570},
	abstract = {Publicaciones de la Organización Mundial de la Salud},
	language = {en},
	note = {Accessed: 2021-09-03},
	keywords = {chartability, accessibility, global vision, vision, WHO, world health organization},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/M8N7FLWY/9789241516570.html:text/html},
	author = {World Health Organization},
	isbn = {9789241516570}
}

@misc{hoang_tableaumagic_2018,
	title = {The {TableauMagic} {DataTables} {Extension}},
	url = {https://tableau.toanhoang.com/the-tableau-magic-datatables-extension-now-available/},
	abstract = {The TableauMagic DataTables Extension v1.0.0 is now available for general use; if you have Tableau Desktop 2018.2 or greater, all you have to do is download the .trex file and start using it in your dashboards today. Note: we have released version 1.10 thanks to support from Tableau and their development team. Usage Create a […]},
	language = {en-US},
	note = {Accessed: 2021-09-03},
	journal = {Toan Hoang},
	author = {Hoang, Toan},
	month = sep,
	year = {2018},
	keywords = {chartability, accessibility, tableau, data table},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/359243J5/the-tableau-magic-datatables-extension-now-available.html:text/html},
}

@misc{noauthor_latest_nodate,
	title = {Latest updates on accessibility in {Tableau}},
	author = {Gupton, Kyle},
	url = {https://www.tableau.com/about/blog/2020/3/latest-updates-accessibility-tableau},
	abstract = {Kyle Gupton shares some notable accessibility developments in Tableau including dashboard accessibility and the accessibility of the Tableau Server and Tableau Online user interface.},
	language = {en-US},
	note = {Accessed: 2021-09-03},
	journal = {Tableau},
	keywords = {chartability, accessibility, tableau},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/64R7LNMH/latest-updates-accessibility-tableau.html:text/html},
}

@misc{noauthor_covid-19_nodate,
	title = {{COVID}-19 data and reports {\textbar} {San} {Francisco}},
	author = {City of San Francisco},
	url = {https://sf.gov/resource/2021/covid-19-data-and-reports},
	note = {Accessed: 2021-09-03},
	keywords = {chartability, accessibility, covid, data, powerbi, san francisco},
	file = {COVID-19 data and reports | San Francisco:/Users/Elavsky/Zotero/storage/GI3SHA5Q/covid-19-data-and-reports.html:text/html},
}

@article{baker_2016,
    author = {Baker, Catherine M. and Milne, Lauren R. and Drapeau, Ryan and Scofield, Jeffrey and Bennett, Cynthia L. and Ladner, Richard E.},
    title = {Tactile Graphics with a Voice},
    year = {2016},
    issue_date = {January 2016},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {8},
    number = {1},
    issn = {1936-7228},
    doi = {10.1145/2854005},
    abstract = {We discuss the development of Tactile Graphics with a Voice (TGV), a system used to access label information in tactile graphics using QR codes. Blind students often rely on tactile graphics to access textbook images. Many textbook images have a large number of text labels that need to be made accessible. In order to do so, we propose TGV, which uses QR codes to replace the text, as an alternative to Braille. The codes are read with a smartphone application. We evaluated the system with a longitudinal study where 10 blind and low-vision participants completed tasks using three different modes on the smartphone application: (1) no guidance, (2) verbal guidance, and (3) finger-pointing guidance. Our results show that TGV is an effective way to access text in tactile graphics, especially for those blind users who are not fluent in Braille. We also found that preferences varied greatly across the modes, indicating that future work should support multiple modes. We expand upon the algorithms we used to implement the finger pointing, algorithms to automatically place QR codes on documents. We also discuss work we have started on creating a Google Glass version of the application.},
    journal = {ACM Trans. Access. Comput.},
    month = {jan},
    articleno = {3},
    numpages = {22},
    keywords = {visually impaired, non-visual feedback, blind, camera, QR codes, Access technology, tactile graphics}
}

@inproceedings{szpiro_2016,
    author = {Szpiro, Sarit Felicia Anais and Hashash, Shafeka and Zhao, Yuhang and Azenkot, Shiri},
    title = {How People with Low Vision Access Computing Devices: Understanding Challenges and Opportunities},
    year = {2016},
    isbn = {9781450341240},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    doi = {10.1145/2982142.2982168},
    abstract = {Low vision is a pervasive condition in which people have difficulty seeing even with corrective lenses. People with low vision frequently use mainstream computing devices, however how they use their devices to access information and whether digital low vision accessibility tools provide adequate support remains understudied. We addressed these questions with a contextual inquiry study. We observed 11 low vision participants using their smartphones, tablets, and computers when performing simple tasks such as reading email. We found that participants preferred accessing information visually than aurally (e.g., screen readers), and juggled a variety of accessibility tools. However, accessibility tools did not provide them with appropriate support. Moreover, participants had to constantly perform multiple gestures in order to see content comfortably. These challenges made participants inefficient-they were slow and often made mistakes; even tech savvy participants felt frustrated and not in control. Our findings reveal the unique needs of low vision people, which differ from those of people with no vision and design opportunities for improving low vision accessibility tools.},
    booktitle = {Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility},
    pages = {171–180},
    numpages = {10},
    keywords = {contextual inquiry, computing devices, accessibility, low vision},
    location = {Reno, Nevada, USA},
    series = {ASSETS '16}
}

@article{lundgard_sociotechnical_2019,
	title = {Sociotechnical {Considerations} for {Accessible} {Visualization} {Design}},
    doi={10.1109/VISUAL.2019.8933762},
    journal={2019 IEEE Visualization Conference (VIS)}, 
	author = {Lundgard, Alan and Lee, Crystal and Satyanarayan, Arvind},
	month = sep,
	year = {2019},
	keywords = {visualization, Computer Science - Computers and Society, accessible, sociotechnical},
	file = {arXiv Fulltext PDF:/Users/Elavsky/Zotero/storage/WABAPUPL/Lundgard et al. - 2019 - Sociotechnical Considerations for Accessible Visua.pdf:application/pdf;arXiv.org Snapshot:/Users/Elavsky/Zotero/storage/VSK35YGI/1909.html:text/html},
}

@misc{demartini_tableau_nodate,
	title = {A {Tableau} {Accessibility} {Journey} - {Part} {II} - {Focus} {Order}},
	url = {https://www.datablick.com/blog/2021/2/18/a-tableau-accessibility-journey-part-ii-focus-order},
	abstract = {This post is a continuation of  A Tableau Accessibility Journey - Part I , which outlines the scope of the work to be covered in this series. The first part of this series includes a number of accessibility issues that were identified in  our case study visualization , and this post will focus on tr},
	language = {en-US},
	note = {Accessed: 2021-09-03},
	journal = {DataBlick},
	author = {DeMartini, Chris},
	keywords = {chartability, accessibility, tableau, focus order},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/QYJWIVYP/a-tableau-accessibility-journey-part-ii-focus-order.html:text/html},
}

@misc{demartini_tableau_nodate-1,
	title = {A {Tableau} {Accessibility} {Journey} - {Part} {IV} - {Keyboard} {Accessibility}},
	url = {https://www.datablick.com/blog/2021/8/10/a-tableau-accessibility-journey-part-iv-keyboard-accessibility},
	abstract = {This post is a continuation of  A Tableau Accessibility Journey - Part III - Color Contrast , earlier posts in the series outline the scope of the work we are covering as part of this effort.  Part I  of this series also includes a number of accessibility issues that were identified in our  case stu},
	language = {en-US},
	note = {Accessed: 2021-09-03},
	journal = {DataBlick},
	author = {DeMartini, Chris},
	keywords = {chartability, accessibility, tableau, keyboard accessibility},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/5QSCSVJL/a-tableau-accessibility-journey-part-iv-keyboard-accessibility.html:text/html},
}

@misc{noauthor_idea_nodate,
	title = {Idea: {Enable} {Keyboard} {Navigation} and {Selection} of {Visualization} {Marks}},
	url = {https://community.tableau.com/s/idea/0874T000000cAp3QAE/detail},
	author = {DeMartini, Chris},
	note = {Accessed: 2021-09-03},
	keywords = {chartability, visualization, accessibility, tableau, voting},
	file = {Idea\: Enable Keyboard Navigation and Selection of Visualization Marks:/Users/Elavsky/Zotero/storage/MN8YGBHK/detail.html:text/html},
}

@inproceedings{mirri_towards_2017,
	title = {Towards accessible graphs in {HTML}-based scientific articles},
	doi = {10.1109/CCNC.2017.7983287},
	abstract = {Recently, several proposals and discussions have tried to push the use of HTML for preparing and sharing research works within the scholarly domain. Therefore, several native HTML-based templates and formats have been introduced for allowing researchers to write scientific documents, which could be access even with assistive technologies, e.g., screen readers. While HTML can be a good basis for guaranteeing accessibility of such papers, several issues are still in place when we consider the inclusion of non-textual entities in the main text, such as complex formulas and figures. In this paper we experiment on the use of one of such HTML formats, i.e., the Research Articles in Simplified HTML (RASH) format, extended for creating accessible graphs automatically out of CSV data. In particular, we introduce an extension to RASH to let the creation of accessible graphs. Moreover, we analyze the outcomes of some preliminary experiments we have done by using different screen readers on several operating systems and browsers.},
	booktitle = {2017 14th {IEEE} {Annual} {Consumer} {Communications} {Networking} {Conference} ({CCNC})},
	author = {Mirri, Silvia and Peroni, Silvio and Salomoni, Paola and Vitali, Fabio and Rubano, Vincenzo},
	month = jan,
	year = {2017},
	note = {ISSN: 2331-9860},
	keywords = {visualization, accessibility, accessibility in education, accessibility in scientific documents, accessible graphs, Assistive technology, Browsers, HTML, Portable document format, RASH, Standards, Tools, Visualization, accessible html graphs, automatic captions},
	pages = {1067--1072},
	file = {IEEE Xplore Full Text PDF:/Users/Elavsky/Zotero/storage/Z5YCA5ST/Mirri et al. - 2017 - Towards accessible graphs in HTML-based scientific.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/Elavsky/Zotero/storage/6U86JYQP/7983287.html:text/html},
}

@inproceedings{Wickham2013BinsummarisesmoothA,
  title={Bin-summarise-smooth : A framework for visualising large data},
  author={Hadley Wickham},
  year={2013}
}

@misc{noauthor_web_nodate,
	title = {Web {Content} {Accessibility} {Guidelines} ({WCAG}) 2.1},
	author = {Web Accessibility Initiative},
	url = {https://www.w3.org/TR/WCAG21/},
	note = {Accessed: 2021-09-03},
	keywords = {chartability, accessibility, standards, wcag 2.1},
	file = {Web Content Accessibility Guidelines (WCAG) 2.1:/Users/Elavsky/Zotero/storage/KZF5H3IL/WCAG21.html:text/html},
}

@inproceedings{sharif_evographs_2018,
	title = {{evoGraphs} — {A} {jQuery} plugin to create web accessible graphs},
	doi = {10.1109/CCNC.2018.8319239},
	abstract = {Graphs are considered as one of the most important tools to represent information, and as such, they are widely used across the Internet in various formats to visualize statistics and data. However, for the visually impaired individuals, who can presently rely only on the ALT attribute of images, receiving and interpreting information displayed in graphs is a highly challenging task. This paper presents a novel solution to this problem, called evoGraphs, which is a jQuery plugin that allows users to create fully dynamic, customizable graphs that are easily read by screen readers. In contrast to traditional images, the proposed approach relies on HTML, CSS and jQuery components in order to create graphs while reducing page load time and making the graphs readable by voiceover software applications and screen readers. This paper presents the design and customization of evo-Graphs. Examples are provided to highlight the advantages of the proposed methodology by comparing page load times of traditional image-based graphs with those produced by evoGraphs.},
	booktitle = {2018 15th {IEEE} {Annual} {Consumer} {Communications} {Networking} {Conference} ({CCNC})},
	author = {Sharif, Ather and Forouraghi, Babak},
	month = jan,
	year = {2018},
	note = {ISSN: 2331-9860},
	keywords = {visualization, accessibility, sharif, Standards, automatic captions, Bars, Cascading style sheets, Conferences, Guidelines, Maintenance engineering, Web pages, evographs},
	pages = {1--4},
	file = {IEEE Xplore Full Text PDF:/Users/Elavsky/Zotero/storage/TP8FU5RQ/Sharif and Forouraghi - 2018 - evoGraphs — A jQuery plugin to create web accessib.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/Elavsky/Zotero/storage/78S4H9IX/8319239.html:text/html},
}

@article{mansur_sound_1985,
	title = {Sound graphs: {A} numerical data analysis method for the blind},
	volume = {9},
	issn = {1573-689X},
	shorttitle = {Sound graphs},
	doi = {10.1007/BF00996201},
	abstract = {A system for the creation of computer-generated sound patterns of two-dimensional line graphs is described. The objectives of the system are to provide the blind with a means of understanding line graphs in the holistic manner used by those with sight. A continuously varying pitch is used to represent motion in the x direction. To test the feasibility of using sound to represent graphs, a prototype system was developed and human factors experimenters were performed. Fourteen subjects were used to compare the tactile-graph methods normally used by the blind to these new sound graphs. It was discovered that mathematical concepts such as symmetry, monotonicity, and the slopes of lines could be determined quickly using sound. Even better performance may be expected with additional training. The flexibility, speed, cost-effectiveness, and greater measure of independence provided the blind or sight-impaired using these methods was demonstrated.},
	language = {en},
	number = {3},
	note = {Accessed: 2021-09-03},
	journal = {Journal of Medical Systems},
	author = {Mansur, Douglass L. and Blattner, Merra M. and Joy, Kenneth I.},
	month = jun,
	year = {1985},
	keywords = {chartability, visualization, accessibility, sonification},
	pages = {163--174},
	file = {Springer Full Text PDF:/Users/Elavsky/Zotero/storage/7FZV4JN7/Mansur et al. - 1985 - Sound graphs A numerical data analysis method for.pdf:application/pdf},
}

@inproceedings{mcgookin_soundbar_2006,
	address = {New York, NY, USA},
	series = {{NordiCHI} '06},
	title = {{SoundBar}: exploiting multiple views in multimodal graph browsing},
	isbn = {978-1-59593-325-6},
	shorttitle = {{SoundBar}},
	doi = {10.1145/1182475.1182491},
	abstract = {In this paper we discuss why access to mathematical graphs is problematic for visually impaired people. By a review of graph understanding theory and interviews with visually impaired users, we explain why current non-visual representations are unlikely to provide effective access to graphs. We propose the use of multiple views of the graph, each providing quick access to specific information as a way to improve graph usability. We then introduce a specific multiple view system to improve access to bar graphs called SoundBar which provides an additional quick audio overview of the graph. An evaluation of SoundBar revealed that additional views significantly increased accuracy and reduced time taken in a question answering task.},
	note = {Accessed: 2021-09-03},
	booktitle = {Proceedings of the 4th {Nordic} conference on {Human}-computer interaction: changing roles},
	publisher = {Association for Computing Machinery},
	author = {McGookin, David K. and Brewster, Stephen A.},
	month = oct,
	year = {2006},
	keywords = {chartability, visualization, accessibility, sonification, haptics, non-speech audio, visual impairment, visualisation},
	pages = {145--154},
}

@article{flowers_cross-modal_1997,
	title = {Cross-{Modal} {Equivalence} of {Visual} and {Auditory} {Scatterplots} for {Exploring} {Bivariate} {Data} {Samples}},
	volume = {39},
	issn = {0018-7208},
	doi = {10.1518/001872097778827151},
	abstract = {The equivalence of visual and auditory scatterplots was examined in two experiments. Experiment 1 examined the relationship between actual Pearson's r and visual and auditory judgments of direction and magnitude of correlation for 24 bivariate data samples. Experiment 2 directly evaluated visual and auditory perceptual sensitivity to outliers by examining changes in perceived magnitude and direction of correlation estimates for scatterplots from Experiment 1 that were altered by the addition of outlier points. Results suggest that the information conveyed by visual and auditory scatterplots is used very similarly by the two modalities. Both visual and auditory scatterplots are quite efficient in conveying sign and magnitude of correlation, and the effect of outliers on judged magnitude of correlation is similar for the two types of data display.},
	language = {en},
	number = {3},
	note = {Accessed: 2021-09-03},
	journal = {Human Factors},
	author = {Flowers, John H. and Buhman, Dion C. and Turnage, Kimberly D.},
	month = sep,
	year = {1997},
	keywords = {chartability, visualization, accessibility, sonification, scatterplot},
	pages = {341--351},
	file = {SAGE PDF Full Text:/Users/Elavsky/Zotero/storage/WMTJDCCA/Flowers et al. - 1997 - Cross-Modal Equivalence of Visual and Auditory Sca.pdf:application/pdf},
}

@article{scoy_auditory_2005,
	title = {{AUDITORY} {AUGMENTATION} {OF} {HAPTIC} {GRAPHS}: {DEVELOPING} {A} {GRAPHIC} {TOOL} {FOR} {TEACHING} {PRECALCULUS} {SKILL} {TO} {BLIND} {STUDENTS}},
	abstract = {This paper discusses the development of a graphic tool to assist in the teaching of pre-calculus skills to blind students. More specifically, it reviews previous and on-going efforts to develop an instrument that will facilitate or enable blind students to examine and explore data and abstract graphs, and other mathematic entities haptically.},
	language = {en},
	author = {Scoy, Frances Van and McLaughlin, Don and Fullmer, Angela},
	year = {2005},
	keywords = {chartability, visualization, accessibility, auditory, haptic},
	pages = {5},
	file = {Scoy et al. - 2005 - AUDITORY AUGMENTATION OF HAPTIC GRAPHS DEVELOPING.pdf:/Users/Elavsky/Zotero/storage/FTRJ42KM/Scoy et al. - 2005 - AUDITORY AUGMENTATION OF HAPTIC GRAPHS DEVELOPING.pdf:application/pdf},
}

@article{tomlinson_exploring_2016,
	title = {Exploring {Auditory} {Graphing} {Software} in the {Classroom}: {The} {Effect} of {Auditory} {Graphs} on the {Classroom} {Environment}},
	volume = {9},
	issn = {1936-7228},
	shorttitle = {Exploring {Auditory} {Graphing} {Software} in the {Classroom}},
	doi = {10.1145/2994606},
	abstract = {Students who are visually impaired make up a population with unique needs for learning. Some tools have been developed to support these needs in the classroom. One such tool, the Graph and Number line Input and Exploration software (GNIE), was developed by the Georgia Institute of Technology Sonification Lab. GNIE was deployed for use in a middle school math classroom at the Georgia Academy for the Blind (GAB) for 2 years starting in fall 2012. We interviewed the middle school math teacher throughout the deployment to learn about the challenges faced when teaching: lesson planning, execution, and review. We also observed how these changed when using GNIE compared to traditional teaching materials. During these 2 years, we conducted interviews and focus groups with students to learn about their attitudes toward tactile graphs compared to auditory graphs. With these in mind, we present lessons learned from the use of GNIE in a real-world classroom and implications for design of software to aid graphical learning for students with vision impairments.},
	number = {1},
	note = {Accessed: 2021-09-03},
	journal = {ACM Transactions on Accessible Computing},
	author = {Tomlinson, Brianna J. and Batterman, Jared and Chew, Yee Chieh and Henry, Ashley and Walker, Bruce N.},
	month = nov,
	year = {2016},
	keywords = {chartability, visualization, accessibility, sonification, haptic, Auditory displays, educational technologies, tacticle},
	pages = {3:1--3:27},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/87YZ2QBI/Tomlinson et al. - 2016 - Exploring Auditory Graphing Software in the Classr.pdf:application/pdf},
}

@inproceedings{yu_haptic_2000,
	title = {Haptic {Graphs} for {Blind} {Computer} {Users}.},
	volume = {2058},
	isbn = {978-3-540-42356-0},
	doi = {10.1007/3-540-44589-7_5},
	abstract = {In this paper we discuss the design of computer-based haptic graphs for blind and visually impaired people with the support of our preliminary experimental results. Since visual impairment makes data visualisation techniques inappropriate for blind people, we are developing a system which can make graphs accessible through haptic and audio media. The disparity between human haptic perception and the sensation simulated by force feedback devices is discussed. Our strategies to tackle technical difficulties posed by the limitations of force feedback devices are explained. Based on the results of experiments conducted on both blind and sighted people, we suggested two techniques: engraving and the use of texture to model curved lines on haptic graphs. Integration of surface property and auditory cues in our system are proposed to assist blind users in exploring haptic graphs.},
	author = {Yu, Wai and Ramloll, Rameshsharma and Brewster, Stephen},
	month = jan,
	year = {2000},
	keywords = {chartability, visualization, accessibility, haptic},
	pages = {41--51},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/KH2BMBIL/Yu et al. - 2000 - Haptic Graphs for Blind Computer Users..pdf:application/pdf},
}

@inproceedings{brown_viztouch_2012,
	address = {New York, NY, USA},
	series = {{TEI} '12},
	title = {{VizTouch}: automatically generated tactile visualizations of coordinate spaces},
	isbn = {978-1-4503-1174-8},
	shorttitle = {{VizTouch}},
	doi = {10.1145/2148131.2148160},
	abstract = {Visual mathematical concepts have long been challenging to access for people with limited or no vision. Given that functions and data plots are typically presented visually; there are few affordable and accessible solutions for individuals with limited or no vision to interpret data in this format. We have developed software that leverages new affordable 3D printing technology to rapidly and automatically generate tactile visualizations. In this paper, we describe development of the VizTouch software through a user-centered design process. We worked with six individuals with low or limited vision to understand the usefulness of 3D printed custom tactile visualizations, and their design. We describe how VizTouch can be used to make data visualizations in education, business, and entertainment accessible.},
	note = {Accessed: 2021-09-03},
	booktitle = {Proceedings of the {Sixth} {International} {Conference} on {Tangible}, {Embedded} and {Embodied} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Brown, Craig and Hurst, Amy},
	month = feb,
	year = {2012},
	keywords = {chartability, visualization, accessibility, human-computer interaction, assistive technology, rapid prototyping, tactile visualizations, tactile},
	pages = {131--138},
}

@inproceedings{hurst_making_2013,
	address = {New York, NY, USA},
	series = {{IDC} '13},
	title = {Making "making" accessible},
	isbn = {978-1-4503-1918-8},
	doi = {10.1145/2485760.2485883},
	abstract = {Assistive technologies empower individuals to accomplish tasks they might not be able to do otherwise. Unfortunately, a large percentage of assistive devices that are purchased (35\% or more) end up unused or abandoned [8], leaving many people with assistive technology that is inappropriate for their needs. This paper describes our ongoing work to help more people gain access to the assistive technology they need by empowering non-engineers to "Do-It-Yourself" (DIY), and thus create, modify, or build their own assistive technology. We discuss how a new generation of rapid prototyping tools and online communities can empower more individuals, and we describe two technologies we have developed to enable novices to prototype and create physical objects.},
	note = {Accessed: 2021-09-03},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Interaction} {Design} and {Children}},
	publisher = {Association for Computing Machinery},
	author = {Hurst, Amy and Kane, Shaun},
	month = jun,
	year = {2013},
	keywords = {visualization, accessibility, assistive technology, rapid prototyping, tactile, do-it-yourself, empowerment, human-centered computing, online communities, personal-scale manufacturing, hacking, making},
	pages = {635--638},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/GK4DM6DU/Hurst and Kane - 2013 - Making making accessible.pdf:application/pdf},
}

@incollection{shi_tickers_2016,
	address = {New York, NY, USA},
	title = {Tickers and {Talker}: {An} {Accessible} {Labeling} {Toolkit} for {3D} {Printed} {Models}},
	isbn = {978-1-4503-3362-7},
	shorttitle = {Tickers and {Talker}},
	doi = {10.1145/2858036.2858507},
	note = {Accessed: 2021-09-03},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Shi, Lei and Zelzer, Idan and Feng, Catherine and Azenkot, Shiri},
	month = may,
	year = {2016},
	keywords = {chartability, accessibility, tactile, acoustic sensing, fabrication, labels, visual impairments, 3d printed, model},
	pages = {4896--4907},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/KUX93CGR/Shi et al. - 2016 - Tickers and Talker An Accessible Labeling Toolkit.pdf:application/pdf},
}

@article{choi_visualizing_2019,
	title = {Visualizing for the {Non}-{Visual}: {Enabling} the {Visually} {Impaired} to {Use} {Visualization}},
	volume = {38},
	issn = {1467-8659},
	shorttitle = {Visualizing for the {Non}-{Visual}},
	doi = {10.1111/cgf.13686},
	abstract = {The majority of visualizations on the web are still stored as raster images, making them inaccessible to visually impaired users. We propose a deep-neural-network-based approach that automatically recognizes key elements in a visualization, including a visualization type, graphical elements, labels, legends, and most importantly, the original data conveyed in the visualization. We leverage such extracted information to provide visually impaired people with the reading of the extracted information. Based on interviews with visually impaired users, we built a Google Chrome extension designed to work with screen reader software to automatically decode charts on a webpage using our pipeline. We compared the performance of the back-end algorithm with existing methods and evaluated the utility using qualitative feedback from visually impaired users.},
	language = {en},
	number = {3},
	note = {Accessed: 2021-09-03},
	journal = {Computer Graphics Forum},
	author = {Choi, Jinho and Jung, Sanghun and Park, Deok Gun and Choo, Jaegul and Elmqvist, Niklas},
	year = {2019},
	keywords = {chartability, visualization, accessibility, • Human-centered computing → Visual analytics, CCS Concepts, Visualization toolkits, dongle},
	pages = {249--260},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/23KZ7TKZ/Choi et al. - 2019 - Visualizing for the Non-Visual Enabling the Visua.pdf:application/pdf;Snapshot:/Users/Elavsky/Zotero/storage/DFNJNBAR/cgf.html:text/html},
}

@incollection{jansen_opportunities_2015,
	address = {New York, NY, USA},
	title = {Opportunities and {Challenges} for {Data} {Physicalization}},
	isbn = {978-1-4503-3145-6},
	doi = {10.1145/2702123.2702180},
	note = {Accessed: 2021-09-03},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Jansen, Yvonne and Dragicevic, Pierre and Isenberg, Petra and Alexander, Jason and Karnik, Abhijit and Kildal, Johan and Subramanian, Sriram and Hornbæk, Kasper},
	month = apr,
	year = {2015},
	keywords = {chartability, visualization, accessibility, tactile, dongle, data physicalization, physical visualization, shape-changing interfaces, tangible user interfaces, physicalization},
	pages = {3227--3236},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/A5HB9D2G/Jansen et al. - 2015 - Opportunities and Challenges for Data Physicalizat.pdf:application/pdf},
}

@inproceedings{bennett_interdependence_2018,
	address = {New York, NY, USA},
	series = {{ASSETS} '18},
	title = {Interdependence as a {Frame} for {Assistive} {Technology} {Research} and {Design}},
	isbn = {978-1-4503-5650-3},
	doi = {10.1145/3234695.3236348},
	abstract = {In this paper, we describe interdependence for assistive technology design, a frame developed to complement the traditional focus on independence in the Assistive Technology field. Interdependence emphasizes collaborative access and people with disabilities' important and often understated contribution in these efforts. We lay the foundation of this frame with literature from the academic discipline of Disability Studies and popular media contributed by contemporary disability justice activists. Then, drawing on cases from our own work, we show how the interdependence frame (1) synthesizes findings from a growing body of research in the Assistive Technology field and (2) helps us orient to additional technology design opportunities. We position interdependence as one possible orientation to, not a prescription for, research and design practice--one that opens new design possibilities and affirms our commitment to equal access for people with disabilities.},
	note = {Accessed: 2021-09-03},
	booktitle = {Proceedings of the 20th {International} {ACM} {SIGACCESS} {Conference} on {Computers} and {Accessibility}},
	publisher = {Association for Computing Machinery},
	author = {Bennett, Cynthia L. and Brady, Erin and Branham, Stacy M.},
	month = oct,
	year = {2018},
	keywords = {accessibility, assistive technology design, interdependence, research},
	pages = {161--173},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/8BRK6XCT/Bennett et al. - 2018 - Interdependence as a Frame for Assistive Technolog.pdf:application/pdf},
}

@inproceedings{jayant_automated_2007,
	address = {New York, NY, USA},
	series = {Assets '07},
	title = {Automated tactile graphics translation: in the field},
	isbn = {978-1-59593-573-1},
	shorttitle = {Automated tactile graphics translation},
	doi = {10.1145/1296843.1296858},
	abstract = {We address the practical problem of automating the process of translating figures from mathematics, science, and engineering textbooks to a tactile form suitable for blind students. The Tactile Graphics Assistant (TGA) and accompanying workflow is described. Components of the TGA that identify text and replace it with Braille use machine learning, computational geometry, and optimization algorithms. We followed through with the ideas in our 2005 paper by creating a more detailed workflow, translating actual images, and analyzing the translation time. Our experience in translating more than 2,300 figures from 4 textbooks demonstrates that figures can be translated in ten minutes or less of human time on average. We describe our experience with training tactile graphics specialists to use the new TGA technology.},
	note = {Accessed: 2021-09-03},
	booktitle = {Proceedings of the 9th international {ACM} {SIGACCESS} conference on {Computers} and accessibility},
	publisher = {Association for Computing Machinery},
	author = {Jayant, Chandrika and Renzelmann, Matt and Wen, Dana and Krisnandi, Satria and Ladner, Richard and Comden, Dan},
	month = oct,
	year = {2007},
	keywords = {chartability, disability, visualization, accessibility, tactile, braille, image processing, machine learning, tactile graphics, user study},
	pages = {75--82},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/AZQJ5G42/Jayant et al. - 2007 - Automated tactile graphics translation in the fie.pdf:application/pdf},
}

@techreport{noauthor_guidelines_nodate,
	title = {Guidelines and {Standards} for {Tactile} {Graphics}},
	author = {BANA},
	institution = {Braille Authority of North America},
	year = {2010},
	type = {Braille {Standard}},
	url = {http://www.brailleauthority.org/tg/},
	note = {Accessed: 2021-09-03},
	keywords = {chartability, visualization, accessibility, tactile, braille, bana},
	file = {Guidelines and Standards for Tactile Graphics:/Users/Elavsky/Zotero/storage/E9WSSBUI/tg.html:text/html},
}

@inproceedings{south_generating_2020,
	title = {Generating {Seizure}-{Inducing} {Sequences} with {Interactive} {Visualizations}},
	booktitle = {Proc. of the IEEE VIS 2020 Posters},
	author = {South, Laura and Borkin, Michelle},
	month = oct,
	year = {2020},
	doi = {10.31219/osf.io/85gwy},
}

@inproceedings{wu_understanding_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {Understanding {Data} {Accessibility} for {People} with {Intellectual} and {Developmental} {Disabilities}},
	isbn = {978-1-4503-8096-6},
	doi = {10.1145/3411764.3445743},
	abstract = {Using visualization requires people to read abstract visual imagery, estimate statistics, and retain information. However, people with Intellectual and Developmental Disabilities (IDD) often process information differently, which may complicate connecting abstract visual information to real-world quantities. This population has traditionally been excluded from visualization design, and often has limited access to data related to their well being. We explore how visualizations may better serve this population. We identify three visualization design elements that may improve data accessibility: chart type, chart embellishment, and data continuity. We evaluate these elements with populations both with and without IDD, measuring accuracy and efficiency in a web-based online experiment with time series and proportion data. Our study identifies performance patterns and subjective preferences for people with IDD when reading common visualizations. These findings suggest possible solutions that may break the cognitive barriers caused by conventional design guidelines.},
	note = {Accessed: 2021-09-03},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Keke and Petersen, Emma and Ahmad, Tahmina and Burlinson, David and Tanis, Shea and Szafir, Danielle Albers},
	month = may,
	year = {2021},
	keywords = {visualization, accessibility, graphical perception \& cognition, human-subjects quantitative studies, cognitive accessibility, idd},
	pages = {1--16},
}

@misc{noauthor_semiotic_nodate,
	title = {Semiotic},
	url = {https://semiotic.nteract.io/guides/accessibility},
	author = {Mazanec, Melanie},
	note = {Accessed: 2021-09-03},
	keywords = {visualization, accessibility, semiotic},
	file = {Semiotic:/Users/Elavsky/Zotero/storage/N6J86GSH/accessibility.html:text/html},
}

@article{balaji_chart-text_2018,
	title = {Chart-{Text}: {A} {Fully} {Automated} {Chart} {Image} {Descriptor}},
	shorttitle = {Chart-{Text}},
	url = {http://arxiv.org/abs/1812.10636},
	note = {Accessed: 2021-09-06},
	journal = {arXiv Preprint},
	author = {Balaji, Abhijit and Ramanathan, Thuvaarakkesh and Sonathi, Venkateshwarlu},
	month = dec,
	year = {2018},
}

@inproceedings{chen_neural_2019,
	address = {New York, NY, USA},
	series = {{UbiComp}/{ISWC} '19 {Adjunct}},
	title = {Neural caption generation over figures},
	isbn = {978-1-4503-6869-8},
	doi = {10.1145/3341162.3345601},
	abstract = {Figures are human-friendly but difficult for computers to process automatically. In this work, we investigate the problem of figure captioning. The goal is to automatically generate a natural language description of a given figure. We create a new dataset for figure captioning, FigCAP. To achieve accurate generation of labels in figures, we propose the Label Maps Attention Model. Extensive experiments show that our method outperforms the baselines. A successful solution to this task allows figure content to be accessible to those with visual impairment by providing input to a text-to-speech system; and enables automatic parsing of vast repositories of documents where figures are pervasive.},
	note = {Accessed: 2021-09-06},
	booktitle = {Adjunct {Proceedings} of the 2019 {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing} and {Proceedings} of the 2019 {ACM} {International} {Symposium} on {Wearable} {Computers}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Charles and Zhang, Ruiyi and Kim, Sungchul and Cohen, Scott and Yu, Tong and Rossi, Ryan and Bunescu, Razvan},
	month = sep,
	year = {2019},
	keywords = {chartability, visualization, accessibility, automatic captions, CNN, figure captioning, LSTM, neural networks, alt text},
	pages = {482--485},
}

@inproceedings{chen_figure_2020,
	title = {Figure {Captioning} with {Relation} {Maps} for {Reasoning}},
	doi = {10.1109/WACV45572.2020.9093592},
	abstract = {Figures, such as line plots, pie charts, bar charts, are widely used to convey important information in a concise format. In this work, we investigate the problem of figure caption generation where the goal is to automatically generate a natural language description for a given figure. While natural image captioning has been studied extensively, figure captioning has received relatively little attention and remains a challenging problem. A successful solution to this task has many potential applications, such as: 1) automatic parsing large amount of figures in PDF document; 2) improving user experience by allowing figure content to be accessible to those with visual impairment. To solve this problem, we introduce a dataset FigCAP and propose novel attention mechanism. In order to solve the exposure bias issue, we further train the captioning model with sequence-level policy based on reinforcement learning, which directly optimizes evaluation metrics. Extensive experiments show that the proposed method outperforms the baselines, thus demonstrating a significant potential for automatic generating captions for figures.},
	booktitle = {2020 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Chen, Charles and Zhang, Ruiyi and Koh, Eunyee and Kim, Sungchul and Cohen, Scott and Rossi, Ryan},
	month = mar,
	year = {2020},
	note = {ISSN: 2642-9381},
	keywords = {chartability, visualization, accessibility, Portable document format, Visualization, automatic captions, Bars, alt text, Computational modeling, Decoding, Task analysis, Training},
	pages = {1526--1534},
	file = {IEEE Xplore Full Text PDF:/Users/Elavsky/Zotero/storage/NKU2ASAV/Chen et al. - 2020 - Figure Captioning with Relation Maps for Reasoning.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/Elavsky/Zotero/storage/LWDQ6MPC/9093592.html:text/html},
}

@incollection{lai_automatic_2020,
	address = {New York, NY, USA},
	title = {Automatic {Annotation} {Synchronizing} with {Textual} {Description} for {Visualization}},
	isbn = {978-1-4503-6708-0},
	doi = {doi.org/10.1145/3313831.3376443},
	abstract = {In this paper, we propose a technique for automatically annotating visualizations according to the textual description. In our approach, visual elements in the target visualization, along with their visual properties, are identified and extracted with a Mask R-CNN model. Meanwhile, the description is parsed to generate visual search requests. Based on the identification results and search requests, each descriptive sentence is displayed beside the described focal areas as annotations. Different sentences are presented in various scenes of the generated animation to promote a vivid step-by-step presentation. With a user-customized style, the animation can guide the audience's attention via proper highlighting such as emphasizing specific features or isolating part of the data. We demonstrate the utility and usability of our method through a user study with use cases.},
	note = {Accessed: 2021-09-06},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lai, Chufan and Lin, Zhixian and Jiang, Ruike and Han, Yun and Liu, Can and Yuan, Xiaoru},
	month = apr,
	year = {2020},
	keywords = {chartability, visualization, accessibility, automatic captions, machine learning, alt text, annotation, natural language interface},
	pages = {1--13},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/EHEUAKTX/Lai et al. - 2020 - Automatic Annotation Synchronizing with Textual De.pdf:application/pdf},
}

@article{obeid_chart--text_2020,
	title = {Chart-to-{Text}: {Generating} {Natural} {Language} {Descriptions} for {Charts} by {Adapting} the {Transformer} {Model}},
	shorttitle = {Chart-to-{Text}},
	url = {http://arxiv.org/abs/2010.09142},
	abstract = {Information visualizations such as bar charts and line charts are very popular for exploring data and communicating insights. Interpreting and making sense of such visualizations can be challenging for some people, such as those who are visually impaired or have low visualization literacy. In this work, we introduce a new dataset and present a neural model for automatically generating natural language summaries for charts. The generated summaries provide an interpretation of the chart and convey the key insights found within that chart. Our neural model is developed by extending the state-of-the-art model for the data-to-text generation task, which utilizes a transformer-based encoder-decoder architecture. We found that our approach outperforms the base model on a content selection metric by a wide margin (55.42\% vs. 8.49\%) and generates more informative, concise, and coherent summaries.},
	note = {Accessed: 2021-09-06},
	journal = {arXiv Preprint},
	author = {Obeid, Jason and Hoque, Enamul},
	month = nov,
	year = {2020},
	keywords = {chartability, visualization, accessibility, automatic captions, alt text, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 10 pages},
	file = {arXiv Fulltext PDF:/Users/Elavsky/Zotero/storage/9R7SJI3F/Obeid and Hoque - 2020 - Chart-to-Text Generating Natural Language Descrip.pdf:application/pdf;arXiv.org Snapshot:/Users/Elavsky/Zotero/storage/RBSD6BKG/2010.html:text/html},
}

@incollection{qian_generating_2021,
	address = {New York, NY, USA},
	title = {Generating {Accurate} {Caption} {Units} for {Figure} {Captioning}},
	isbn = {978-1-4503-8312-7},
	doi = {10.1145/3442381.3449923},
	abstract = {Scientific-style figures are commonly used on the web to present numerical information. Captions that tell accurate figure information and sound natural would significantly improve figure accessibility. In this paper, we present promising results on machine figure captioning. A recent corpus analysis of real-world captions reveals that machine figure captioning systems should start by generating accurate caption units. We formulate the caption unit generation problem as a controlled captioning problem. Given a caption unit type as a control signal, a model generates an accurate caption unit of that type. As a proof-of-concept on single bar charts, we propose a model, FigJAM, that achieves this goal through utilizing metadata information and a joint static and dynamic dictionary. Quantitative evaluations with two datasets from the figure question answering task show that our model can generate more accurate caption units than competitive baseline models. A user study with ten human experts confirms the value of machine-generated caption units in their standalone accuracy and naturalness. Finally, a post-editing simulation study demonstrates the potential for models to paraphrase and stitch together single-type caption units into multi-type captions by learning from data.},
	note = {Accessed: 2021-09-06},
	booktitle = {Proceedings of the {Web} {Conference} 2021},
	publisher = {Association for Computing Machinery},
	author = {Qian, Xin and Koh, Eunyee and Du, Fan and Kim, Sungchul and Chan, Joel and Rossi, Ryan A. and Malik, Sana and Lee, Tak Yeon},
	month = apr,
	year = {2021},
	keywords = {chartability, visualization, accessibility, automatic captions, alt text, Data visualization, figure question answering, image captioning, text generation, web accessibility},
	pages = {2792--2804},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/QD84UKB7/Qian et al. - 2021 - Generating Accurate Caption Units for Figure Capti.pdf:application/pdf},
}

@inproceedings{mankoff_disability_2010,
	address = {New York, NY, USA},
	series = {{ASSETS} '10},
	title = {Disability studies as a source of critical inquiry for the field of assistive technology},
	isbn = {978-1-60558-881-0},
	doi = {10.1145/1878803.1878807},
	abstract = {Disability studies and assistive technology are two related fields that have long shared common goals - understanding the experience of disability and identifying and addressing relevant issues. Despite these common goals, there are some important differences in what professionals in these fields consider problems, perhaps related to the lack of connection between the fields. To help bridge this gap, we review some of the key literature in disability studies. We present case studies of two research projects in assistive technology and discuss how the field of disability studies influenced that work, led us to identify new or different problems relevant to the field of assistive technology, and helped us to think in new ways about the research process and its impact on the experiences of individuals who live with disability. We also discuss how the field of disability studies has influenced our teaching and highlight some of the key publications and publication venues from which our community may want to draw more deeply in the future.},
	note = {Accessed: 2021-09-06},
	booktitle = {Proceedings of the 12th international {ACM} {SIGACCESS} conference on {Computers} and accessibility},
	publisher = {Association for Computing Machinery},
	author = {Mankoff, Jennifer and Hayes, Gillian R. and Kasnitz, Devva},
	month = oct,
	year = {2010},
	keywords = {chartability, accessibility, assistive technology, disability studies, self-advocacy},
	pages = {3--10},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/6E5FPH9M/Mankoff et al. - 2010 - Disability studies as a source of critical inquiry.pdf:application/pdf},
}

@article{martinez_methodology_2021,
	title = {Methodology for heuristic evaluation of the accessibility of statistical charts for people with low vision and color vision deficiency},
	author = {Martínez, Rubén Alcaraz and Turró, Mireia Ribera and Saltiveri, Toni Granollers},
	month = dec,
	year = {2021},
	doi = {10.21203/rs.3.rs-156959/v1},
	journal = {Universal Access in the Information Society},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/PRJ5Y2TQ/Martínez et al. - 2021 - Methodology for heuristic evaluation of the access.pdf:application/pdf},
}

@inproceedings{aldrich_talk_2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Talk to the {Hand}: {An} {Agenda} for {Further} {Research} on {Tactile} {Graphics}},
	isbn = {978-3-540-87730-1},
	shorttitle = {Talk to the {Hand}},
	doi = {10.1007/978-3-540-87730-1_31},
	abstract = {Tactile graphics are the primary means by which blind people access maps, graphs, diagrams and other graphical representations. Tactile graphics are made up of raised lines, areas, textures and symbols, and are intended to be felt rather than seen [1], [2] and [3].Major obstacles to the successful use of tactile graphics are that: touch cannot discriminate the fine detail that sight can; extracting information through a sequence of touches, then re-integrating it, imposes a heavy memory load; and many graphical representations need visual experience for interpretation.},
	language = {en},
	booktitle = {Diagrammatic {Representation} and {Inference}},
	publisher = {Springer},
	author = {Aldrich, Frances},
	editor = {Stapleton, Gem and Howse, John and Lee, John},
	year = {2008},
	keywords = {visualization, accessibility, tactile graphics},
	pages = {344--346},
	file = {Springer Full Text PDF:/Users/Elavsky/Zotero/storage/JHUEKZGS/Aldrich - 2008 - Talk to the Hand An Agenda for Further Research o.pdf:application/pdf},
}

@misc{schneider_constructing_nodate,
	title = {Constructing the {Yellow} {Brick} {Road}: {Route} {Bricks} on {Virtual} {Tactile} {Maps}},
	shorttitle = {Constructing the {Yellow} {Brick} {Road}},
	abstract = {Tactile maps are the standard media to convey geographical information to blind people. An approach for an interactive substitute for tactile maps is presented, called virtual tactile maps. It is based on an image processing system tracking fingers and game piece-like objects. Output is done through speech and sound. Users learn routes by constructing them with route bricks. 1.},
	author = {Schneider, Jochen},
	keywords = {chartability, visualization, accessibility, tactile graphics},
	file = {Citeseer - Full Text PDF:/Users/Elavsky/Zotero/storage/8QJEEUF2/Schneider - Constructing the Yellow Brick Road Route Bricks o.pdf:application/pdf;Citeseer - Snapshot:/Users/Elavsky/Zotero/storage/2AMCKI2Q/summary.html:text/html},
}

@incollection{morris_rich_2018,
	address = {New York, NY, USA},
	title = {Rich {Representations} of {Visual} {Content} for {Screen} {Reader} {Users}},
	isbn = {978-1-4503-5620-6},
	doi = {10.1145/3173574.3173633},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Morris, Meredith Ringel and Johnson, Jazette and Bennett, Cynthia L. and Cutrell, Edward},
	month = apr,
	year = {2018},
	keywords = {chartability, visualization, accessibility, visual impairment, alt text, alternative text, blindness, captions, screen readers},
	pages = {1--11},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/A8YA2LYR/Morris et al. - 2018 - Rich Representations of Visual Content for Screen .pdf:application/pdf},
}

@inproceedings{butler_technology_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {Technology {Developments} in {Touch}-{Based} {Accessible} {Graphics}: {A} {Systematic} {Review} of {Research} 2010-2020},
	isbn = {978-1-4503-8096-6},
	shorttitle = {Technology {Developments} in {Touch}-{Based} {Accessible} {Graphics}},
	doi = {10.1145/3411764.3445207},
	abstract = {This paper presents a systematic literature review of 292 publications from 97 unique venues on touch-based graphics for people who are blind or have low vision, from 2010 to mid-2020. It is the first review of its kind on touch-based accessible graphics. It is timely because it allows us to assess the impact of new technologies such as commodity 3D printing and low-cost electronics on the production and presentation of accessible graphics. As expected our review shows an increase in publications from 2014 that we can attribute to these developments. It also reveals the need to: broaden application areas, especially to the workplace; broaden end-user participation throughout the full design process; and conduct more in situ evaluation. This work is linked to an online living resource to be shared with the wider community.},
	note = {Accessed: 2021-09-06},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Butler, Matthew and Holloway, Leona M and Reinders, Samuel and Goncu, Cagatay and Marriott, Kim},
	month = may,
	year = {2021},
	keywords = {chartability, visualization, accessibility, tactile graphics, Assistive Technology, Blind, Low Vision, Systematic Literature Review, Tactile Graphics, review},
	pages = {1--15},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/8BQV6GAJ/Butler et al. - 2021 - Technology Developments in Touch-Based Accessible .pdf:application/pdf},
}

@book{geldard_tactual_1983,
	title = {Tactual {Perception}: {A} {Source} {Book}},
	shorttitle = {Tactual {Perception}},
	doi = {10.2307/1422824},
	publisher = {Cambridge University Press},
	abstract = {Editorial preface William Schiff and Emerson Foulke 1. Tactual perception in historical perspective: David Katz's world of touch Lester E. Krueger 2. The psychophysics of touch Carl E. Sherrick and James C. Craig 3. The development of haptic perception David H. Warren 4. The perception of texture by touch Susan J. Lederman 5. Reading braille Emerson Foulke 6. Dynamic tactile displays James C. Craig and Carl E. Sherrick 7. Current developments in tactile communication of speech Jacob H. Kirman 8. Social touching Stephen Thayer 9. Haptic pictures John M. Kennedy 10. Mobility maps Grahame A. James 11. Haptic perception of tangible graphic displays Edward P. Berla 12. Tangible graphic displays in the education of blind persons Billie L. Bentzen 13. Production of tangible graphic displays John M. Gill 14. Tangible graphics: producers' views Jasha M. Levi and Nancy S. Amick 15. A user's view of tangible graphics: The Louisville Workshop William Schiff Name index Subject index.},
	author = {Geldard, F. A. and Schiff, W. and Foulke, E.},
	year = {1983},
	keywords = {chartability, visualization, accessibility, tactile perception},
}

@article{lederman_spatial_1985,
	title = {Spatial and movement-based heuristics for encoding pattern information through touch.},
	doi = {10.1037//0096-3445.114.1.33},
	abstract = {Seven experiments investigated the heuristics people use to encode spatial pattern information through touch. Observers traced a tangible pathway with one hand and then answered questions about either the euclidean line between the pathway endpoints or the pathway itself. Parameters of the euclidean line were held constant, while characteristics of the felt pathway were manipulated. Experiments 1-4 showed that blindfolded sighted and blind observers increasingly overestimated the length of the euclidean line as the length of the explored pathway increased. This indicates a movement-based heuristic for encoding distance. Experiments 5-7 indicated that judgments of the position of the euclidean line did not vary with the position of the felt pathway or the extent to which it deviated from that line. Instead, the results indicated that observers relied on implicit spatial axes, which are movement independent, to judge position. These and other results have implications for theories of haptic encoding of spatial pattern and for the construction of tangible graphics displays.},
	journal = {Journal of experimental psychology. General},
	author = {Lederman, S. and Klatzky, R. and Barber, P. O.},
	year = {1985},
	file = {Full Text:/Users/Elavsky/Zotero/storage/FADUVKRP/Lederman et al. - 1985 - Spatial and movement-based heuristics for encoding.pdf:application/pdf},
}

@article{gallace_what_2011,
	title = {To what extent do {Gestalt} grouping principles influence tactile perception?},
	doi = {10.1037/a0022335},
	abstract = {Since their formulation by the Gestalt movement more than a century ago, the principles of perceptual grouping have primarily been investigated in the visual modality and, to a lesser extent, in the auditory modality. The present review addresses the question of whether the same grouping principles also affect the perception of tactile stimuli. Although, to date, only a few studies have explicitly investigated the existence of Gestalt grouping principles in the tactile modality, we argue that many more studies have indirectly provided evidence relevant to this topic. Reviewing this body of research, we argue that similar principles to those reported previously in visual and auditory studies also govern the perceptual grouping of tactile stimuli. In particular, we highlight evidence showing that the principles of proximity, similarity, common fate, good continuation, and closure affect tactile perception in both unimodal and crossmodal settings. We also highlight that the grouping of tactile stimuli is often affected by visual and auditory information that happen to be presented simultaneously. Finally, we discuss the theoretical and applied benefits that might pertain to the further study of Gestalt principles operating in both unisensory and multisensory tactile perception.},
	journal = {Psychological bulletin},
	author = {Gallace, A. and Spence, C.},
	year = {2011},
	keywords = {chartability, visualization, accessibility, tactile, gestalt, perception},
}

@article{lederman_perception_1986,
	title = {Perception of texture by vision and touch: multidimensionality and intersensory integration.},
	shorttitle = {Perception of texture by vision and touch},
	doi = {10.1037//0096-1523.12.2.169},
	abstract = {A series of six experiments offers converging evidence that there is no fixed dominance hierarchy for the perception of textured patterns, and in doing so, highlights the importance of recognizing the multidimensionality of texture perception. The relative bias between vision and touch was reversed or considerably altered using both discrepancy and nondiscrepancy paradigms. This shift was achieved merely by directing observers to judge different dimensions of the same textured surface. Experiments 1, 4, and 5 showed relatively strong emphasis on visual as opposed to tactual cues regarding the spatial density of raised dot patterns. In contrast, Experiments 2, 3, and 6 demonstrated considerably greater emphasis on the tactual as opposed to visual cues when observers were instructed to judge the roughness of the same surfaces. The results of the experiments were discussed in terms of a modality appropriateness interpretation of intersensory bias. A weighted averaging model appeared to describe the nature of the intersensory integration process for both spatial density and roughness perception.},
	journal = {Journal of experimental psychology. Human perception and performance},
	author = {Lederman, S. and Thorne, G. and Jones, B.},
	year = {1986},
	keywords = {chartability, visualization, accessibility, tactile, perception, intersensory, multidimensionality, multisensory},
}

@inproceedings{cullen_co-designing_2019,
	address = {New York, NY, USA},
	series = {{IDC} '19},
	title = {Co-designing {Inclusive} {Multisensory} {Story} {Mapping} with {Children} with {Mixed} {Visual} {Abilities}},
	isbn = {978-1-4503-6690-8},
	doi = {10.1145/3311927.3323146},
	abstract = {Story mapping is used in schools to promote children's understanding of stories and narrative structure. As a collaborative activity, it can support creativity and facilitate group interaction. However, most techniques used in primary schools rely on visual materials, which creates a barrier to learning for children with visual impairments (VI). To address this, we set out to design a collaborative story mapping tool with a group of children with mixed visual abilities and their teaching assistants. Using co-design approaches over ten workshops, we designed and prototyped different ideas for engaging children in storytelling and design. We present our co-design process and findings, and the resulting story mapping system. We outline how using multisensory elements can facilitate creativity and collaboration to help children with mixed visual abilities create and share stories together, and support learning and social inclusion of VI children in mainstream classrooms.},
	note = {Accessed: 2021-09-06},
	booktitle = {Proceedings of the 18th {ACM} {International} {Conference} on {Interaction} {Design} and {Children}},
	publisher = {Association for Computing Machinery},
	author = {Cullen, Clare and Metatla, Oussama},
	month = jun,
	year = {2019},
	keywords = {chartability, visualization, accessibility, multisensory, Children, Co-Design, Education, Inclusion, Mixed Abilities, Multisensory, Storytelling, Tangibles, co-design},
	pages = {361--373},
}

@inproceedings{bornschein_collaborative_2015,
	address = {New York, NY, USA},
	series = {{ASSETS} '15},
	title = {Collaborative {Creation} of {Digital} {Tactile} {Graphics}},
	isbn = {978-1-4503-3400-6},
	doi = {10.1145/2700648.2809869},
	booktitle = {Proceedings of the 17th {International} {ACM} {SIGACCESS} {Conference} on {Computers} \& {Accessibility}},
	publisher = {Association for Computing Machinery},
	author = {Bornschein, Jens and Prescher, Denise and Weber, Gerhard},
	month = oct,
	year = {2015},
	keywords = {chartability, visualization, accessibility, evaluation, design, tactile, tactile graphics, blind users, collaboration, drawing-application, pin-matrix device},
	pages = {117--126},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/UUCPQJSJ/Bornschein et al. - 2015 - Collaborative Creation of Digital Tactile Graphics.pdf:application/pdf},
}

@incollection{bennett_promise_2019,
	address = {New York, NY, USA},
	title = {The {Promise} of {Empathy}: {Design}, {Disability}, and {Knowing} the "{Other}"},
	isbn = {978-1-4503-5970-2},
	shorttitle = {The {Promise} of {Empathy}},
	doi = {10.1145/3290605.3300528},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Bennett, Cynthia L. and Rosner, Daniela K.},
	month = may,
	year = {2019},
	keywords = {disability, accessibility, design, design methods, empathy, othering},
	pages = {1--13},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/PVEDIC7J/Bennett and Rosner - 2019 - The Promise of Empathy Design, Disability, and Kn.pdf:application/pdf},
}

@inproceedings{tigwell_nuanced_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {Nuanced {Perspectives} {Toward} {Disability} {Simulations} from {Digital} {Designers}, {Blind}, {Low} {Vision}, and {Color} {Blind} {People}},
	isbn = {978-1-4503-8096-6},
	doi = {10.1145/3411764.3445620},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Tigwell, Garreth W.},
	month = may,
	year = {2021},
	keywords = {Accessibility, accessibility, design, empathy, digital design, disability simulations., disability simulation},
	pages = {1--15},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/V885M4R4/Tigwell - 2021 - Nuanced Perspectives Toward Disability Simulations.pdf:application/pdf},
}

@techreport{initiative_wai_accessibility_nodate,
    type = {{WCAG} {Standard}},
    key = {WCAG Principles},
	author = {WAI},
	year = {2019},
	institution = {{W3C}},
	title = {Accessibility Principles},
	url = {https://www.w3.org/WAI/fundamentals/accessibility-principles/},
	note = {Accessed: 2021-09-06},
}

@techreport{noauthor_study_2021,
	title = {{Automated} {Testing} {Identifies} 57 {Percent} of {Digital} {Accessibility} {Issues}},
	url = {https://www.deque.com/blog/automated-testing-study-identifies-57-percent-of-digital-accessibility-issues/},
	language = {en-US},
	note = {Accessed: 2021-09-06},
	author = {Deque},
	institution = {Deque},
	month = mar,
	year = {2021},
	keywords = {chartability, accessibility, deque, coverage},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/P62PSHGH/automated-testing-study-identifies-57-percent-of-digital-accessibility-issues.html:text/html},
}

@misc{noauthor_why_nodate,
	title = {Why {Accessibility} {Is} at the {Heart} of {Data} {Visualization} {\textbar} by {Doug} {Schepers} {\textbar} {Nightingale} {\textbar} {Medium}},
	url = {https://medium.com/nightingale/accessibility-is-at-the-heart-of-data-visualization-64a38d6c505b},
	author = {Schepers, Doug},
	note = {Accessed: 2021-09-06},
	keywords = {chartability, visualization, accessibility, assistive technology},
	file = {Why Accessibility Is at the Heart of Data Visualization | by Doug Schepers | Nightingale | Medium:/Users/Elavsky/Zotero/storage/C9843KJR/accessibility-is-at-the-heart-of-data-visualization-64a38d6c505b.html:text/html},
}

@incollection{bigham_vizwiz_2010,
	address = {New York, NY, USA},
	title = {{VizWiz}: nearly real-time answers to visual questions},
	isbn = {978-1-4503-0271-5},
	shorttitle = {{VizWiz}},
	doi = {10.1145/1866029.1866080},
	abstract = {The lack of access to visual information like text labels, icons, and colors can cause frustration and decrease independence for blind people. Current access technology uses automatic approaches to address some problems in this space, but the technology is error-prone, limited in scope, and quite expensive. In this paper, we introduce VizWiz, a talking application for mobile phones that offers a new alternative to answering visual questions in nearly real-time - asking multiple people on the web. To support answering questions quickly, we introduce a general approach for intelligently recruiting human workers in advance called quikTurkit so that workers are available when new questions arrive. A field deployment with 11 blind participants illustrates that blind people can effectively use VizWiz to cheaply answer questions in their everyday lives, highlighting issues that automatic approaches will need to address to be useful. Finally, we illustrate the potential of using VizWiz as part of the participatory design of advanced tools by using it to build and evaluate VizWiz::LocateIt, an interactive mobile tool that helps blind people solve general visual search problems.},
	note = {Accessed: 2021-09-06},
	booktitle = {Proceedings of the 23nd annual {ACM} symposium on {User} interface software and technology},
	publisher = {Association for Computing Machinery},
	author = {Bigham, Jeffrey P. and Jayant, Chandrika and Ji, Hanjie and Little, Greg and Miller, Andrew and Miller, Robert C. and Miller, Robin and Tatarowicz, Aubrey and White, Brandyn and White, Samual and Yeh, Tom},
	month = oct,
	year = {2010},
	keywords = {chartability, accessibility, alt text, blind users, collaboration, non-visual interfaces, real-time human computation, crowdsourcing, description, realtime, visual},
	pages = {333--342},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/CL2HS7G4/Bigham et al. - 2010 - VizWiz nearly real-time answers to visual questio.pdf:application/pdf},
}

@inproceedings{moraes_evaluating_2014,
	address = {New York, NY, USA},
	series = {{ASSETS} '14},
	title = {Evaluating the accessibility of line graphs through textual summaries for visually impaired users},
	isbn = {978-1-4503-2720-6},
	doi = {10.1145/2661334.2661368},
	abstract = {This paper presents the methodology for generating textual summaries of line graphs in the SIGHT (Summarizing Information GrapHics Textually) system and the evaluation of line graph summaries produced by SIGHT. The system is designed to deliver the high-level knowledge conveyed by informational graphics present in online popular media articles (newspaper and magazines) to individuals who do not have visual access to the image. It works by producing and delivering a concise summary of the graph's content including the most important visual features present in the graphic. The system is briefly described; the evaluation compares the utility of the generated textual summaries to visually viewing the graphic in order to answer important questions about the line graph.},
	booktitle = {Proceedings of the 16th international {ACM} {SIGACCESS} conference on {Computers} \& accessibility},
	publisher = {Association for Computing Machinery},
	author = {Moraes, Priscilla and Sina, Gabriel and McCoy, Kathleen and Carberry, Sandra},
	month = oct,
	year = {2014},
	keywords = {chartability, visualization, accessibility, visual impairment, visual impairments, information graphics, natural language generation, descriptions, line graph},
	pages = {83--90},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/G5AR3QM5/Moraes et al. - 2014 - Evaluating the accessibility of line graphs throug.pdf:application/pdf},
}

@article{chaparro_applications_2017,
	title = {Applications of {Color} in {Design} for {Color}-{Deficient} {Users}},
	volume = {25},
	issn = {1064-8046},
	doi = {10.1177/1064804616635382},
	abstract = {Color vision deficiency is common, affecting one in every 12 men. Despite its prevalence, displays are seldom designed to accommodate color-vision-deficient (CVD) users, who confront daily challenges interpreting color in a broad range of applications, whether weather displays, informational graphics, road signs, or computer interfaces. In this article we discuss the prevalence of color deficiency, its effects, and the availability of tools that enable design teams to evaluate candidate solutions that meet the needs of CVD users, thereby ensuring universal accessibility.},
	language = {en},
	number = {1},
	note = {Accessed: 2021-09-06},
	journal = {Ergonomics in Design},
	author = {Chaparro, Alex and Chaparro, Maria},
	month = jan,
	year = {2017},
	keywords = {chartability, visualization, accessibility, usability, color blindness, color vision deficiency, computer interface, redundant color coding, universal design, color, colorblindness, cvd},
	pages = {23--30},
	file = {SAGE PDF Full Text:/Users/Elavsky/Zotero/storage/MN58YM9P/Chaparro and Chaparro - 2017 - Applications of Color in Design for Color-Deficien.pdf:application/pdf},
}

@article{nunez_optimizing_2018,
	title = {Optimizing colormaps with consideration for color vision deficiency to enable accurate interpretation of scientific data},
	volume = {13},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0199239},
	language = {en},
	number = {7},
	journal = {PLOS ONE},
	author = {Nuñez, Jamie R. and Anderton, Christopher R. and Renslow, Ryan S.},
	month = aug,
	year = {2018},
}

@article{oliveira_towards_2013,
	title = {Towards {More} {Accessible} {Visualizations} for {Color}-{Vision}-{Deficient} {Individuals}},
	volume = {15},
	issn = {1558-366X},
	doi = {10.1109/MCSE.2013.113},
	abstract = {Various techniques can be used to improve visualization experiences for individuals with color vision deficiency, including recoloring, pattern superposition, and the use of a color-perception simulation model for assisting visualization designers.},
	number = {5},
	journal = {Computing in Science Engineering},
	author = {Oliveira, Manuel M.},
	month = sep,
	year = {2013},
	note = {Conference Name: Computing in Science Engineering},
	keywords = {chartability, visualization, accessibility, design, color vision deficiency, cvd, Color vision, color-perception simulation, CVD, Pattern recognition, pattern superposition, Perception, recoloring, scientific computing, Scientific computing},
	pages = {80--87},
	file = {IEEE Xplore Abstract Record:/Users/Elavsky/Zotero/storage/I5GPF987/6673983.html:text/html;IEEE Xplore Full Text PDF:/Users/Elavsky/Zotero/storage/MBRERS3B/Oliveira - 2013 - Towards More Accessible Visualizations for Color-V.pdf:application/pdf},
}

@inproceedings{schaadhardt_understanding_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {Understanding {Blind} {Screen}-{Reader} {Users}\&\#x2019; {Experiences} of {Digital} {Artboards}},
	isbn = {978-1-4503-8096-6},
	doi = {10.1145/3411764.3445242},
	note = {Accessed: 2021-09-06},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Schaadhardt, Anastasia and Hiniker, Alexis and Wobbrock, Jacob O.},
	month = may,
	year = {2021},
}

@article{zhao_data_2008,
	title = {Data {Sonification} for {Users} with {Visual} {Impairment}: {A} {Case} {Study} with {Georeferenced} {Data}},
	volume = {15},
	issn = {1073-0516},
	shorttitle = {Data {Sonification} for {Users} with {Visual} {Impairment}},
	doi = {10.1145/1352782.1352786},
	abstract = {We describe the development and evaluation of a tool, iSonic, to assist users with visual impairment in exploring georeferenced data using coordinated maps and tables, augmented with nontextual sounds and speech output. Our in-depth case studies with 7 blind users during 42 hours of data collection, showed that iSonic enabled them to find facts and discover trends in georeferenced data, even in unfamiliar geographical contexts, without special devices. Our design was guided by an Action-by-Design-Component (ADC) framework, which was also applied to scatterplots to demonstrate its generalizability. Video and download is available at www.cs.umd.edu/hcil/iSonic/.},
	number = {1},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Zhao, Haixia and Plaisant, Catherine and Shneiderman, Ben and Lazar, Jonathan},
	month = may,
	year = {2008},
}

@article{brewster_visualization_2002,
	title = {Visualization tools for blind people using multiple modalities},
	volume = {24},
	issn = {0963-8288},
	doi = {10.1080/09638280110111388},
	abstract = {PURPOSE: There are many problems when blind people need to access visualizations such as graphs and tables. Current speech or raised-paper technology does not provide a good solution. Our approach is to use non-speech sounds and haptics to allow a richer and more flexible form of access to graphs and tables.
METHOD: Two experiments are reported that test out designs for both sound and haptic graph solutions. In the audio case a standard speech interface is compared to one with non-speech sounds added. The haptic experiment compares two different graph designs to see which was the most effective.
RESULTS: Our results for the sound graphs showed a significant decrease in subjective workload, reduced time taken to complete tasks and reduced errors as compared to a standard speech interface. For the haptic graphs reductions in workload and some of the problems that can occur when using such graphs are shown.
CONCLUSIONS: Using non-speech sound and haptics can significantly improve interaction with visualizations such as graphs. This multimodal approach makes the most of the senses our users have to provide access to information in more flexible ways.},
	language = {eng},
	number = {11-12},
	journal = {Disability and Rehabilitation},
	author = {Brewster, S.},
	month = aug,
	year = {2002},
	pmid = {12182801},
	keywords = {chartability, visualization, accessibility, sonification, Blindness, Equipment Design, Ergonomics, Female, Humans, Male, Pilot Projects, Research Design, Self-Help Devices, Sensitivity and Specificity, Software Design, Software Validation, Sound, Sound Localization, Touch, User-Computer Interface, Visual Perception},
	pages = {613--621},
	file = {Accepted Version:/Users/Elavsky/Zotero/storage/FWZAVCZG/Brewster - 2002 - Visualization tools for blind people using multipl.pdf:application/pdf},
}

@misc{noauthor_accessibility_nodate,
	title = {Highcharts Accessibility Module},
	url = {https://highcharts.com/docs/accessibility/accessibility-module},
	abstract = {Accessibility module},
	author = {Highsoft},
	language = {en},
	note = {Accessed: 2021-09-06},
}

@misc{vcc,
	title = {Visa {Chart} {Components}},
	url = {https://github.com/visa/visa-chart-components},
	abstract = {Contribute to visa/visa-chart-components development by creating an account on GitHub.},
	author = {Visa},
	language = {en},
	note = {Accessed: 2021-09-06},
	journal = {GitHub},
	keywords = {chartability, visualization, accessibility, visa chart components},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/TBJJHPXP/utils.html:text/html},
}

@misc{noauthor_revealing_nodate,
	title = {Revealing {Room} for {Improvement} in {Accessibility} within a {Social} {Media} {Data} {Visualization} {Learning} {Community}},
	url = {https://silvia.rbind.io/talk/2021-05-04-data-viz-accessibility/},
	language = {en},
	note = {Accessed: 2021-09-06},
	author = {Canelón, Silvia and Hare, Liz},
	keywords = {chartability, visualization, accessibility, libraries},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/JCCDQ49U/2021-05-04-data-viz-accessibility.html:text/html},
}

@misc{noauthor_making_2018,
	title = {Making {Shiny} apps accessible for all humans},
	url = {https://community.rstudio.com/t/making-shiny-apps-accessible-for-all-humans/8458},
	language = {en},
	note = {Accessed: 2021-09-06},
	author = {RStudio Community},
	month = may,
	year = {2018},
	keywords = {chartability, visualization, accessibility, libraries},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/D4TCMLYX/8458.html:text/html},
}

@misc{noauthor_are_2018,
	title = {Are plotly tables accessible?},
	url = {https://community.plotly.com/t/are-plotly-tables-accessible/8263},
	abstract = {For our visualizations we need to have a table view of data for accessibility purposes. I am wondering if the tables generated by plotly meet web accessibility requirements.},
	language = {en},
	note = {Accessed: 2021-09-06},
	author = {Plotly Community Forum},
	month = feb,
	year = {2018},
	keywords = {chartability, visualization, accessibility, libraries},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/WZVRDZFZ/8263.html:text/html},
}

@misc{noauthor_solved_2019,
	title = {{Datatables} and {Accessibility}},
	url = {https://community.plotly.com/t/solved-datatables-and-accessibility/31085},
	language = {en},
	note = {Accessed: 2021-09-06},
	author = {Plotly Community Forum},
	month = nov,
	year = {2019},
	keywords = {chartability, visualization, accessibility, libraries},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/IDJK7C9V/31085.html:text/html},
}

@misc{simon_making_2020,
	title = {Making {Graphs} {And} {Plots} {Accessible} {For} {The} {Blind}},
	url = {https://andadapt.com/making-graphs-and-plots-accessible-for-the-blind/},
	language = {en-US},
	note = {Accessed: 2021-09-06},
	author = {Wheatcroft, Simon},
	month = jun,
	year = {2020},
	keywords = {chartability, visualization, accessibility, libraries},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/LTYGHT85/making-graphs-and-plots-accessible-for-the-blind.html:text/html},
}

@misc{noauthor_power_nodate,
	title = {Power {BI} {Accessibility} {Best} {Practices}},
	url = {https://rklein324.github.io/PowerBIAccessibility/},
	language = {en-US},
	author = {Klein, Rebecca},
	note = {Accessed: 2021-09-06},
	journal = {PowerBIAccessibility},
	keywords = {chartability, visualization, accessibility, powerbi},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/B97H45B8/PowerBIAccessibility.html:text/html},
}

@misc{noauthor_mapboxmapbox-gl-accessibility_2021,
	title = {Mapbox Gl Accessibility},
	copyright = {ISC},
	url = {https://github.com/mapbox/mapbox-gl-accessibility},
	abstract = {An accessibility control for Mapbox GL JS},
	note = {Accessed: 2021-09-06},
	author = {Mapbox},
	month = aug,
	year = {2021},
}

@article{brangier_beyond_2018,
	title = {Beyond "{Usability} and {User} {Experience}" , {Towards} an {Integrative} {Heuristic} {Inspection}: from {Accessibility} to {Persuasiveness} in the {UX} {Evaluation} {A} {Case} {Study} on an {Insurance} {Prospecting} {Tablet} {Application}},
	journal = {Advances in Usability and User Experience},
	shorttitle = {Beyond "{Usability} and {User} {Experience}" , {Towards} an {Integrative} {Heuristic} {Inspection}},
	language = {en},
	author = {Brangier, Eric and Urrutia, Josefina Gil and Senderowicz, Véronique and Cessat, Laurent},
	month = jun,
	year = {2018},
    doi = {10.1007/978-3-319-60492-3_44},
    isbn = {978-3-319-60492-3},
}

@misc{noauthor_unlocking_2018,
	title = {Unlocking {Accessibility} for {UX}/{UI} {Designers}},
	url = {https://www.24a11y.com/2018/unlocking-accessibility-for-ux-ui-designers/},
	abstract = {In today's article, Denis Boudreau explains that web accessibility is hard, complicated and you don’t need to care about all of that WCAG stuff.},
	language = {en-US},
	author = {Boudreau, Denis},
	note = {Accessed: 2021-09-07},
	journal = {24 Accessibility},
	month = dec,
	year = {2018},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/K7KKFFX8/unlocking-accessibility-for-ux-ui-designers.html:text/html},
}

@misc{experience_10_nodate,
	title = {10 {Usability} {Heuristics} for {User} {Interface} {Design}},
	url = {https://www.nngroup.com/articles/ten-usability-heuristics/},
	author = {Nielsen, Jakob},
	abstract = {Jakob Nielsen's 10 general principles for interaction design. They are called "heuristics" because they are broad rules of thumb and not specific usability guidelines.},
	language = {en},
	note = {Accessed: 2021-09-07},
	journal = {Nielsen Norman Group},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/NUJY6AWC/ten-usability-heuristics.html:text/html},
}

@incollection{nielsen_heuristic_1994,
	address = {USA},
	title = {Heuristic evaluation},
	isbn = {978-0-471-01877-3},
	note = {Accessed: 2021-09-06},
	booktitle = {Usability inspection methods},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Nielsen, Jakob},
	month = jun,
	year = {1994},
	pages = {25--62},
}

@inproceedings{nielsen_heuristic_1990,
	address = {New York, NY, USA},
	series = {{CHI} '90},
	title = {Heuristic evaluation of user interfaces},
	isbn = {978-0-201-50932-8},
	doi = {10.1145/97243.97281},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Nielsen, Jakob and Molich, Rolf},
	month = mar,
	year = {1990},
	pages = {249--256},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/W42H9RII/Nielsen and Molich - 1990 - Heuristic evaluation of user interfaces.pdf:application/pdf},
}

@inproceedings{slavkovic_novice_1999,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '99},
	title = {Novice heuristic evaluations of a complex interface},
	isbn = {978-1-58113-158-1},
	doi = {10.1145/632716.632902},
	booktitle = {{CHI} '99 {Extended} {Abstracts} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Slavkovic, Aleksandra and Cross, Karen},
	month = may,
	year = {1999},
	keywords = {heuristic evaluation, usability evaluation methods},
	pages = {304--305},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/RLBYQZ6B/Slavkovic and Cross - 1999 - Novice heuristic evaluations of a complex interfac.pdf:application/pdf},
}

@inproceedings{otey_methodology_2017,
	address = {New York, NY, USA},
	series = {Interacción '17},
	title = {A methodology to develop usability / user experience heuristics},
	isbn = {978-1-4503-5229-1},
	doi = {10.1145/3123818.3133832},
	booktitle = {Proceedings of the {XVIII} {International} {Conference} on {Human} {Computer} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Otey, Daniela Quiñones},
	month = sep,
	year = {2017},
	keywords = {usability, user experience, heuristic evaluation, methodology, usability heuristics},
	pages = {1--2},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/9CHFKG4G/Otey - 2017 - A methodology to develop usability  user experien.pdf:application/pdf},
}

@inproceedings{joyce_mobile_2016,
	address = {Cham},
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {Mobile {Application} {Usability}: {Heuristic} {Evaluation} and {Evaluation} of {Heuristics}},
	isbn = {978-3-319-41935-0},
	shorttitle = {Mobile {Application} {Usability}},
	doi = {10.1007/978-3-319-41935-0_8},
	abstract = {Many traditional usability evaluation methods do not consider mobile-specific issues. This can result in mobile applications that abound in usability issues. We empirically evaluate three sets of usability heuristics for use with mobile applications, including a set defined by the authors. While the set of heuristics defined by the authors surface more usability issues in a mobile application than other sets of heuristics, improvements to the set can be made.},
	language = {en},
	booktitle = {Advances in {Human} {Factors}, {Software}, and {Systems} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Joyce, Ger and Lilley, Mariana and Barker, Trevor and Jefferies, Amanda},
	editor = {Amaba, Ben},
	year = {2016},
	keywords = {Heuristic evaluation, Human factors, Mobile apps, Usability},
	pages = {77--86},
	file = {Springer Full Text PDF:/Users/Elavsky/Zotero/storage/YNU6SBTN/Joyce et al. - 2016 - Mobile Application Usability Heuristic Evaluation.pdf:application/pdf},
}

@inproceedings{santos_heuristic_2018,
	title = {Heuristic {Evaluation} in {Visualization}: {An} {Empirical} {Study} : {Position} paper},
	shorttitle = {Heuristic {Evaluation} in {Visualization}},
	doi = {10.1109/BELIV.2018.8634108},
	abstract = {Heuristic evaluation is a usability inspection method that has been adapted to evaluate visualization applications through the development of specific sets of heuristics. This paper presents an empirical study meant to assess the capacity of the method to anticipate the usability issues noticed by users when using a visualization application. The potential usability problems identified by 20 evaluators were compared with the issues found for the same application by 46 users through a usability test, as well as with the fixes recommended by the experimenters observing those users during the test. Results suggest that using some heuristics may have elicited potential problems that none of the users noticed while using the application; on the other hand, users encountered unpredicted usability issues.},
	booktitle = {2018 {IEEE} {Evaluation} and {Beyond} - {Methodological} {Approaches} for {Visualization} ({BELIV})},
	author = {Santos, Beatriz Sousa and Silva, Samuel and Dias, Paulo},
	month = oct,
	year = {2018},
	keywords = {Tools, Visualization, Data visualization, heuristic evaluation, Usability, empirical study, heuristics sets, Inspection, Systematics, Visualization evaluation},
	pages = {78--85},
	file = {IEEE Xplore Full Text PDF:/Users/Elavsky/Zotero/storage/FUXSAWDI/Santos et al. - 2018 - Heuristic Evaluation in Visualization An Empirica.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/Elavsky/Zotero/storage/3G9IGQML/8634108.html:text/html},
}

@misc{noauthor_chartability_nodate,
	title = {Chartability},
	author = {Elavsky, Frank},
	url = {https://chartability.github.io/POUR-CAF/},
	note = {Accessed: 2021-09-07},
}

@techreport{noauthor_understanding_nodate,
    type = {{WCAG} {Standard}},
    key = {WCAG Principles},
	author = {WAI},
	title = {1.4.1 Use of Color},
	url = {https://www.w3.org/TR/UNDERSTANDING-WCAG20/visual-audio-contrast-without-color.html},
	note = {Accessed: 2021-09-07},
}

@misc{noauthor_inclusive_nodate,
	title = {Inclusive {Design} {Principles}},
	author = {Swan, Henny and Pouncey, Ian and Pickering, Heydon and Watson, Léonie},
	url = {https://inclusivedesignprinciples.org/},
	note = {Accessed: 2021-09-07},
	file = {Inclusive Design Principles:/Users/Elavsky/Zotero/storage/IZRNIF2Y/inclusivedesignprinciples.org.html:text/html},
}

@book{card_readings_1999,
	title = {Readings in {Information} {Visualization}: {Using} {Vision} {To} {Think}},
	isbn = {978-1-55860-533-6},
	shorttitle = {Readings in {Information} {Visualization}},
	author = {Card, Stuart and Mackinlay, Jock and Shneiderman, Ben},
	month = jan,
	year = {1999},
	note = {Journal Abbreviation: Information Visualization - IVS
Publication Title: Information Visualization - IVS},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/7P7ADM6K/Card et al. - 1999 - Readings in Information Visualization Using Visio.pdf:application/pdf},
}

@article{wobbrock_ability-based_2011,
	title = {Ability-{Based} {Design}: {Concept}, {Principles} and {Examples}},
	volume = {3},
	issn = {1936-7228},
	shorttitle = {Ability-{Based} {Design}},
	doi = {10.1145/1952383.1952384},
	abstract = {Current approaches to accessible computing share a common goal of making technology accessible to users with disabilities. Perhaps because of this goal, they may also share a tendency to centralize disability rather than ability. We present a refinement to these approaches called ability-based design that consists of focusing on ability throughout the design process in an effort to create systems that leverage the full range of human potential. Just as user-centered design shifted the focus of interactive system design from systems to users, ability-based design attempts to shift the focus of accessible design from disability to ability. Although prior approaches to accessible computing may consider users’ abilities to some extent, ability-based design makes ability its central focus. We offer seven ability-based design principles and describe the projects that inspired their formulation. We also present a research agenda for ability-based design.},
	number = {3},
	note = {Accessed: 2021-09-07},
	journal = {ACM Transactions on Accessible Computing},
	author = {Wobbrock, Jacob O. and Kane, Shaun K. and Gajos, Krzysztof Z. and Harada, Susumu and Froehlich, Jon},
	month = apr,
	year = {2011},
	keywords = {assistive technology, universal design, universal usability, Ability-based design, adaptive user interfaces, computer access, design for all, inclusive design, user interfaces for all},
	pages = {9:1--9:27},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/2MY44LAI/Wobbrock et al. - 2011 - Ability-Based Design Concept, Principles and Exam.pdf:application/pdf},
}

@misc{noauthor_sas_nodate,
	title = {{SAS} {Graphics} {Accelerator}},
	type = {Product},
	url = {https://support.sas.com/software/products/graphics-accelerator/},
	note = {Accessed: 2021-09-08},
	author = {SAS},
}

@incollection{miesenberger_accessible_2018,
	address = {Cham},
	title = {An {Accessible} {Interaction} {Model} for {Data} {Visualisation} in {Statistics}},
	volume = {10896},
	isbn = {978-3-319-94276-6 978-3-319-94277-3},
	doi = {10.1007/978-3-319-94277-3_92},
	language = {en},
	note = {Accessed: 2021-10-12},
	booktitle = {Computers {Helping} {People} with {Special} {Needs}},
	publisher = {Springer International Publishing},
	author = {Godfrey, A. Jonathan R. and Murrell, Paul and Sorge, Volker},
	editor = {Miesenberger, Klaus and Kouroupetroglou, Georgios},
	year = {2018},
	pages = {590--597},
	file = {Godfrey et al. - 2018 - An Accessible Interaction Model for Data Visualisa.pdf:/Users/Elavsky/Zotero/storage/N5E5WKYD/Godfrey et al. - 2018 - An Accessible Interaction Model for Data Visualisa.pdf:application/pdf},
}

@inproceedings{sorge_polyfilling_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Polyfilling {Accessible} {Chemistry} {Diagrams}},
	isbn = {978-3-319-41264-1},
	doi = {10.1007/978-3-319-41264-1_6},
	language = {en},
	booktitle = {Computers {Helping} {People} with {Special} {Needs}},
	publisher = {Springer International Publishing},
	author = {Sorge, Volker},
	editor = {Miesenberger, Klaus and Bühler, Christian and Penaz, Petr},
	year = {2016},
	keywords = {Chemistry diagrams, STEM accessibility, Web accessibility},
	pages = {43--50},
	file = {Springer Full Text PDF:/Users/Elavsky/Zotero/storage/QIKLATI8/Sorge - 2016 - Polyfilling Accessible Chemistry Diagrams.pdf:application/pdf},
}

@article{quinones_methodology_2018,
	title = {A methodology to develop usability/user experience heuristics},
	volume = {59},
	issn = {0920-5489},
	doi = {10.1016/j.csi.2018.03.002},
	abstract = {Technology, software systems and human–computer interaction paradigms are evolving. Traditional usability heuristics do not cover all aspects of user–system interactions. Many sets of heuristics have been proposed, with the aim of evaluating specific application domains and their specific usability-related features. In addition, several sets of heuristics are used to evaluate aspects other than usability that are related to the user experience (UX). However, most authors use an informal process to develop usability/UX heuristics; there is no clear protocol for heuristic validation. This can result in sets of usability/UX heuristics that are difficult to understand or use; moreover, the resulting sets of heuristics may not be effective or efficient evaluation tools. This article presents a formal methodology for developing usability/user experience heuristics. The methodology was applied in practice in several case studies; it was also validated through expert opinions.},
	language = {en},
	note = {Accessed: 2021-10-29},
	journal = {Computer Standards \& Interfaces},
	author = {Quiñones, Daniela and Rusu, Cristian and Rusu, Virginica},
	month = aug,
	year = {2018},
	keywords = {Heuristic evaluation, User experience, Usability, Methodology, Usability heuristics},
	pages = {109--129},
	file = {ScienceDirect Snapshot:/Users/Elavsky/Zotero/storage/PT2PA3YW/S0920548917303860.html:text/html},
}

@inproceedings{ahmetovic_audiofunctionsweb_2019,
	address = {New York, NY, USA},
	series = {{W4A} '19},
	title = {{AudioFunctions}.web: {Multimodal} {Exploration} of {Mathematical} {Function} {Graphs}},
	isbn = {978-1-4503-6716-5},
	shorttitle = {{AudioFunctions}.web},
	doi = {10.1145/3315002.3317560},
	abstract = {We present AudioFunctions.web, a web app that uses sonification, earcons and speech synthesis to enable blind people to explore mathematical function graphs. The system is designed for personalized access through different interfaces (touchscreen, keyboard, touchpad and mouse) on both mobile and traditional devices, in order to better adapt to different user abilities and preferences. It is also publicly available as a web service and can be directly accessed from the teaching material through a hypertext link. An experimental evaluation with 13 visually impaired participants highlights that, while the usability of all the presented interaction modalities is high, users with different abilities prefer different interfaces to interact with the system. It is also shown that users with higher level of mathematical education are capable of better adapting to interaction modalities considered more difficult by others.},
	booktitle = {Proceedings of the 16th {International} {Web} for {All} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Ahmetovic, Dragan and Bernareggi, Cristian and Guerreiro, João and Mascetti, Sergio and Capietto, Anna},
	month = may,
	year = {2019},
	keywords = {Function graphs, Mathematics, Visual Impairments and Blindness},
	pages = {1--10},
	file = {Full Text:/Users/Elavsky/Zotero/storage/DHD6JQYW/Ahmetovic et al. - 2019 - AudioFunctions.web Multimodal Exploration of Math.pdf:application/pdf},
}

@inproceedings{power_2012,
    author = {Power, Christopher and Freire, Andr\'{e} and Petrie, Helen and Swallow, David},
    title = {Guidelines Are Only Half of the Story: Accessibility Problems Encountered by Blind Users on the Web},
    year = {2012},
    isbn = {9781450310154},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    doi = {10.1145/2207676.2207736},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    pages = {433–442},
    numpages = {10},
    keywords = {accessibility guidelines, web accessibility, blind users, user evaluation},
    location = {Austin, Texas, USA},
    series = {CHI '12}
}

@inproceedings{rodrigues_data_2018,
	address = {Montreal QC Canada},
	title = {Data {Donors}: {Sharing} {Knowledge} for {Mobile} {Accessibility}},
	isbn = {978-1-4503-5621-3},
	shorttitle = {Data {Donors}},
	doi = {10.1145/3170427.3188627},
	abstract = {Individuals regularly face challenges when interacting with their mobile devices particularly if they are not tech savvy users. When such difficulties occur, individuals often rely on more knowledgeable users to overcome difficulties. However, many do not have a support network of knowledgeable individuals available. Moreover, some challenges go beyond the need for guidance, as for example difficulties in performing swipes for motor impaired people. In this paper, we propose Data Donors, a conceptual framework proposing the enablement of users with the capacity to help others to do so by donating their mobile interaction data and knowledge. Inspired by charitable donations, we outline the Data Donors framework and discuss three applications that are being developed under the data donning paradigm. Through this work, we will explore the consequences and opportunities of sharing ones’ data for the greater good and discuss the creation of global data donation programs.},
	language = {en},
	note = {Accessed: 2021-11-22},
	booktitle = {Extended {Abstracts} of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Rodrigues, André and Montague, Kyle and Guerreiro, Tiago},
	month = apr,
	year = {2018},
	pages = {1--6},
	file = {Rodrigues et al. - 2018 - Data Donors Sharing Knowledge for Mobile Accessib.pdf:/Users/Elavsky/Zotero/storage/VLWAAETV/Rodrigues et al. - 2018 - Data Donors Sharing Knowledge for Mobile Accessib.pdf:application/pdf},
}

@article{carvalho_audio-based_nodate,
	title = {Audio-{Based} {Puzzle} {Gaming} for {Blind} {People}},
	abstract = {Mobile devices provide opportunities beyond communication and productivity. Particularly, they are increasingly used to entertain the nomadic user. Mobile casual games are increasingly popular as their simplicity, lack of commitment required and neglect for particular special skills goes hand-in-hand with the reduced and partitioned time spans associated with daily contexts. Gaming possibilities for blind people are scarce. The extra demands imposed by mobile interfaces and contexts augments this exclusion. Given the paucity of mobile games for blind people along with the recognized benefits of such gaming experiences, we present an audio-based puzzle game for blind people. In this paper, we depict the iterative (participatory) design of the game along with a preliminary evaluation with 13 blind participants. Results show that the game is fun and challenging and suggest that playable counterparts of visualbased games are feasible even in restrictive contexts as is the mobile one.},
	language = {en},
	author = {Carvalho, Jaime and Guerreiro, Tiago and Duarte, Luis and Carriço, Luis},
	pages = {8},
	file = {Carvalho et al. - Audio-Based Puzzle Gaming for Blind People.pdf:/Users/Elavsky/Zotero/storage/Z9LREAUP/Carvalho et al. - Audio-Based Puzzle Gaming for Blind People.pdf:application/pdf},
}

@inproceedings{rello_dytective_2016,
	address = {Montreal Canada},
	title = {Dytective: towards detecting dyslexia across languages using an online game},
	isbn = {978-1-4503-4138-7},
	shorttitle = {Dytective},
	doi = {10.1145/2899475.2899491},
	abstract = {At least 10\% of the global population has dyslexia. In the United States and Spain, dyslexia is associated with a large percentage of school drop out. Current methods to detect risk of dyslexia are language speciﬁc, expensive, or do not scale well because they require a professional or extensive equipment. A central challenge to detecting dyslexia is han­ dling its diﬀering manifestations across languages. To ad­ dress this, we designed a browser-based game, Dytective, to detect risk of dyslexia across the English and Spanish lan­ guages. Dytective consists of linguistic tasks informed by analysis of common errors made by persons with dyslexia. To evaluate Dytective, we conducted a user study with 60 English and Spanish speaking children between 7 and 12 years old. We found children with and without dyslexia dif­ fered signiﬁcantly in their performance on the game. Our results suggest that Dytective is able to diﬀerentiate school age children with and without dyslexia in both English and Spanish speakers.},
	language = {en},
	note = {Accessed: 2021-11-22},
	booktitle = {Proceedings of the 13th {International} {Web} for {All} {Conference}},
	publisher = {ACM},
	author = {Rello, Luz and Williams, Kristin and Ali, Abdullah and White, Nancy Cushen and Bigham, Jeffrey P.},
	month = apr,
	year = {2016},
	pages = {1--4},
	file = {Rello et al. - 2016 - Dytective towards detecting dyslexia across langu.pdf:/Users/Elavsky/Zotero/storage/DFE57XAE/Rello et al. - 2016 - Dytective towards detecting dyslexia across langu.pdf:application/pdf},
}


@inproceedings{rheingans_perceptual_1995,
	address = {Berlin, Heidelberg},
	series = {{IFIP} {Series} on {Computer} {Graphics}},
	title = {Perceptual {Principles} for {Effective} {Visualizations}},
	isbn = {978-3-642-79057-7},
	doi = {10.1007/978-3-642-79057-7_6},
	abstract = {Since visual data representations are perceived through the filter of the human visual system, it is imperative that the characteristics of this system be taken into account during the design and rendering of visual displays. This paper presents a set of perceptual guidelines for the construction of effective visualizations. In most cases, side-by side pictures demonstrate the impact of the suggested techniques.},
	language = {en},
	booktitle = {Perceptual {Issues} in {Visualization}},
	publisher = {Springer},
	author = {Rheingans, Penny and Landreth, Chris},
	editor = {Grinstein, Georges and Levkowitz, Haim},
	year = {1995},
	keywords = {Circular Object, Color Scale, Human Visual System, Ozone Surface, Spectrum Scale},
	pages = {59--73},
}

@article{kim_accessible_2021,
	title = {Accessible {Visualization}: {Design} {Space}, {Opportunities}, and {Challenges}},
	volume = {40},
	issn = {1467-8659},
	shorttitle = {Accessible {Visualization}},
	doi = {10.1111/cgf.14298},
	abstract = {Visualizations are now widely used across disciplines to understand and communicate data. The benefit of visualizations lies in leveraging our natural visual perception. However, the sole dependency on vision can produce unintended discrimination against people with visual impairments. While the visualization field has seen enormous growth in recent years, supporting people with disabilities is much less explored. In this work, we examine approaches to support this marginalized user group, focusing on visual disabilities. We collected and analyzed papers published for the last 20 years on visualization accessibility. We mapped a design space for accessible visualization that includes seven dimensions: user group, literacy task, chart type, interaction, information granularity, sensory modality, assistive technology. We described the current knowledge gap in light of the latest advances in visualization and presented a preliminary accessibility model by synthesizing findings from existing research. Finally, we reflected on the dimensions and discussed opportunities and challenges for future research.},
	language = {en},
	number = {3},
	journal = {Computer Graphics Forum},
	author = {Kim, N. W. and Joyner, S. C. and Riegelhuth, A. and Kim, Y.},
	year = {2021},
	keywords = {Accessibility, CCS Concepts, • Human-centered computing → Visualization},
	pages = {173--188},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/IMMD84W7/Kim et al. - 2021 - Accessible Visualization Design Space, Opportunit.pdf:application/pdf;Snapshot:/Users/Elavsky/Zotero/storage/QFBRMWMF/cgf.html:text/html},
}

@article{marriott_inclusive_2021,
	title = {Inclusive data visualization for people with disabilities: a call to action},
	volume = {28},
	issn = {1072-5520},
	shorttitle = {Inclusive data visualization for people with disabilities},
	doi = {10.1145/3457875},
	number = {3},
	journal = {Interactions},
	author = {Marriott, Kim and Lee, Bongshin and Butler, Matthew and Cutrell, Ed and Ellis, Kirsten and Goncu, Cagatay and Hearst, Marti and McCoy, Kathleen and Szafir, Danielle Albers},
	month = apr,
	year = {2021},
	pages = {47--51},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/AZ88YCVP/Marriott et al. - 2021 - Inclusive data visualization for people with disab.pdf:application/pdf},
}

@ARTICLE{lundgard_accessible_22,
  author={Lundgard, Alan and Satyanarayan, Arvind},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content}, 
  year={2022},
  volume={28},
  number={1},
  pages={1073-1083},
  doi={10.1109/TVCG.2021.3114770}
 }

@misc{noauthor_extensive_2016,
	title = {Extensive digitization of tactile map collection},
	author = {Hale, Jen},
	url = {https://www.perkins.org/extensive-digitization-of-tactile-map-collection/},
	abstract = {Over 100 tactile maps from the 1830's to the 1960's have just been added to the Perkins Archives digital map collection.},
	language = {en-US},
	note = {Perkins Archives Blog, Perkins School for the Blind. July 2016. Accessed: 2022-02-23},
	journal = {Perkins School for the Blind},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/5ZSD9ZEX/extensive-digitization-of-tactile-map-collection.html:text/html},
}

@article{jung_communicating_2022,
	title = {Communicating {Visualizations} without {Visuals}: {Investigation} of {Visualization} {Alternative} {Text} for {People} with {Visual} {Impairments}},
	volume = {28},
	issn = {1077-2626},
	shorttitle = {Communicating {Visualizations} without {Visuals}},
	doi = {10.1109/TVCG.2021.3114846},
	abstract = {Alternative text is critical in communicating graphics to people who are blind or have low vision. Especially for graphics that contain rich information, such as visualizations, poorly written or an absence of alternative texts can worsen the information access inequality for people with visual impairments. In this work, we consolidate existing guidelines and survey current practices to inspect to what extent current practices and recommendations are aligned. Then, to gain more insight into what people want in visualization alternative texts, we interviewed 22 people with visual impairments regarding their experience with visualizations and their information needs in alternative texts. The study findings suggest that participants actively try to construct an image of visualizations in their head while listening to alternative texts and wish to carry out visualization tasks (e.g., retrieve specific values) as sighted viewers would. The study also provides ample support for the need to reference the underlying data instead of visual elements to reduce users\&\#x0027; cognitive burden. Informed by the study, we provide a set of recommendations to compose an informative alternative text.},
	language = {English},
	number = {01},
	note = {Accessed: 2022-02-23},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Jung, Crescentia and Mehta, Shubham and Kulkarni, Atharva and Zhao, Yuhang and Kim, Yea-Seul},
	month = jan,
	year = {2022},
	pages = {1095--1105},
	file = {Submitted Version:/Users/Elavsky/Zotero/storage/XQBLFP4Q/Jung et al. - 2022 - Communicating Visualizations without Visuals Inve.pdf:application/pdf;Snapshot:/Users/Elavsky/Zotero/storage/C5NEZASR/1xjQYJDwaxa.html:text/html},
}

@article{chundury_towards_2022,
	title = {Towards {Understanding} {Sensory} {Substitution} for {Accessible} {Visualization}: {An} {Interview} {Study}},
	volume = {28},
	issn = {1941-0506},
	shorttitle = {Towards {Understanding} {Sensory} {Substitution} for {Accessible} {Visualization}},
	doi = {10.1109/TVCG.2021.3114829},
	abstract = {For all its potential in supporting data analysis, particularly in exploratory situations, visualization also creates barriers: accessibility for blind and visually impaired individuals. Regardless of how effective a visualization is, providing equal access for blind users requires a paradigm shift for the visualization research community. To enact such a shift, it is not sufficient to treat visualization accessibility as merely another technical problem to overcome. Instead, supporting the millions of blind and visually impaired users around the world who have equally valid needs for data analysis as sighted individuals requires a respectful, equitable, and holistic approach that includes all users from the onset. In this paper, we draw on accessibility research methodologies to make inroads towards such an approach. We first identify the people who have specific insight into how blind people perceive the world: orientation and mobility (O\&M) experts, who are instructors that teach blind individuals how to navigate the physical world using non-visual senses. We interview 10 O\&M experts-all of them blind-to understand how best to use sensory substitution other than the visual sense for conveying spatial layouts. Finally, we investigate our qualitative findings using thematic analysis. While blind people in general tend to use both sound and touch to understand their surroundings, we focused on auditory affordances and how they can be used to make data visualizations accessible-using sonification and auralization. However, our experts recommended supporting a combination of senses-sound and touch-to make charts accessible as blind individuals may be more familiar with exploring tactile charts. We report results on both sound and touch affordances, and conclude by discussing implications for accessible visualization for blind individuals.},
	language = {eng},
	number = {1},
	journal = {IEEE transactions on visualization and computer graphics},
	author = {Chundury, Pramod and Patnaik, Biswaksen and Reyazuddin, Yasmin and Tang, Christine and Lazar, Jonathan and Elmqvist, Niklas},
	month = jan,
	year = {2022},
	pmid = {34587061},
	keywords = {Blindness, Humans, Touch, Computer Graphics, Vision, Ocular, Visually Impaired Persons},
	pages = {1084--1094},
}

@inproceedings{mack_what_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {What {Do} {We} {Mean} by \&\#x201c;{Accessibility} {Research}\&\#x201d;? {A} {Literature} {Survey} of {Accessibility} {Papers} in {CHI} and {ASSETS} from 1994 to 2019},
	isbn = {978-1-4503-8096-6},
	shorttitle = {What {Do} {We} {Mean} by \&\#x201c;{Accessibility} {Research}\&\#x201d;?},
	doi = {10.1145/3411764.3445412},
	abstract = {Accessibility research has grown substantially in the past few decades, yet there has been no literature review of the field. To understand current and historical trends, we created and analyzed a dataset of accessibility papers appearing at CHI and ASSETS since ASSETS’ founding in 1994. We qualitatively coded areas of focus and methodological decisions for the past 10 years (2010-2019, N=506 papers), and analyzed paper counts and keywords over the full 26 years (N=836 papers). Our findings highlight areas that have received disproportionate attention and those that are underserved—for example, over 43\% of papers in the past 10 years are on accessibility for blind and low vision people. We also capture common study characteristics, such as the roles of disabled and nondisabled participants as well as sample sizes (e.g., a median of 13 for participant groups with disabilities and older adults). We close by critically reflecting on gaps in the literature and offering guidance for future work in the field.},
	note = {Accessed: 2022-02-22},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Mack, Kelly and McDonnell, Emma and Jain, Dhruv and Lu Wang, Lucy and E. Froehlich, Jon and Findlater, Leah},
	month = may,
	year = {2021},
	keywords = {disability, Accessibility, assistive technology, literature review},
	pages = {1--18},
	file = {Submitted Version:/Users/Elavsky/Zotero/storage/GHECXUIA/Mack et al. - 2021 - What Do We Mean by &#x201c\;Accessibility Research&.pdf:application/pdf},
}

@article{borkin_beyond_2016,
	title = {Beyond {Memorability}: {Visualization} {Recognition} and {Recall}},
	volume = {22},
	issn = {1941-0506},
	shorttitle = {Beyond {Memorability}},
	doi = {10.1109/TVCG.2015.2467732},
	abstract = {In this paper we move beyond memorability and investigate how visualizations are recognized and recalled. For this study we labeled a dataset of 393 visualizations and analyzed the eye movements of 33 participants as well as thousands of participant-generated text descriptions of the visualizations. This allowed us to determine what components of a visualization attract people's attention, and what information is encoded into memory. Our findings quantitatively support many conventional qualitative design guidelines, including that (1) titles and supporting text should convey the message of a visualization, (2) if used appropriately, pictograms do not interfere with understanding and can improve recognition, and (3) redundancy helps effectively communicate the message. Importantly, we show that visualizations memorable “at-a-glance” are also capable of effectively conveying the message of the visualization. Thus, a memorable visualization is often also an effective one.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Borkin, Michelle A. and Bylinskii, Zoya and Kim, Nam Wook and Bainbridge, Constance May and Yeh, Chelsea S. and Borkin, Daniel and Pfister, Hanspeter and Oliva, Aude},
	month = jan,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Atmospheric measurements, Data visualization, Encoding, eye-tracking study, Information visualization, memorability, Particle measurements, recall, recognition, Redundancy, Target recognition, Visualization},
	pages = {519--528},
	file = {IEEE Xplore Abstract Record:/Users/Elavsky/Zotero/storage/M7AIB8XE/7192646.html:text/html},
}

@article{franconeri_science_2021,
	title = {The {Science} of {Visual} {Data} {Communication}: {What} {Works}},
	volume = {22},
	issn = {1529-1006},
	shorttitle = {The {Science} of {Visual} {Data} {Communication}},
	doi = {10.1177/15291006211051956},
	abstract = {Effectively designed data visualizations allow viewers to use their powerful visual systems to understand patterns in data across science, education, health, and public policy. But ineffectively designed visualizations can cause confusion, misunderstanding, or even distrust—especially among viewers with low graphical literacy. We review research-backed guidelines for creating effective and intuitive visualizations oriented toward communicating data to students, coworkers, and the general public. We describe how the visual system can quickly extract broad statistics from a display, whereas poorly designed displays can lead to misperceptions and illusions. Extracting global statistics is fast, but comparing between subsets of values is slow. Effective graphics avoid taxing working memory, guide attention, and respect familiar conventions. Data visualizations can play a critical role in teaching and communication, provided that designers tailor those visualizations to their audience.},
	language = {en},
	number = {3},
	note = {Accessed: 2022-03-02},
	journal = {Psychological Science in the Public Interest},
	author = {Franconeri, Steven L. and Padilla, Lace M. and Shah, Priti and Zacks, Jeffrey M. and Hullman, Jessica},
	month = dec,
	year = {2021},
	note = {Publisher: SAGE Publications Inc},
	keywords = {data visualization, graph comprehension, reasoning, statistical cognition, uncertainty communication, visual communication},
	pages = {110--161},
	file = {SAGE PDF Full Text:/Users/Elavsky/Zotero/storage/UHETYMT8/Franconeri et al. - 2021 - The Science of Visual Data Communication What Wor.pdf:application/pdf},
}

@article{padilla_uncertain_2021,
	title = {Uncertain {About} {Uncertainty}: {How} {Qualitative} {Expressions} of {Forecaster} {Confidence} {Impact} {Decision}-{Making} {With} {Uncertainty} {Visualizations}},
	volume = {11},
	issn = {1664-1078},
	shorttitle = {Uncertain {About} {Uncertainty}},
	doi = {10.3389/fpsyg.2020.579267},
	abstract = {When forecasting events, multiple types of uncertainty are often inherently present in the modeling process. Various uncertainty typologies exist, and each type of uncertainty has different implications a scientist might want to convey. In this work, we focus on one type of distinction between direct quantitative uncertainty and indirect qualitative uncertainty. Direct quantitative uncertainty describes uncertainty about facts, numbers, and hypotheses that can be communicated in absolute quantitative forms such as probability distributions or confidence intervals. Indirect qualitative uncertainty describes the quality of knowledge concerning how effectively facts, numbers, or hypotheses represent reality, such as evidence confidence scales proposed by the Intergovernmental Panel on Climate Change. A large body of research demonstrates that both experts and novices have difficulty reasoning with quantitative uncertainty, and visualizations of uncertainty can help with such traditionally challenging concepts. However, the question of if, and how, people may reason with multiple types of uncertainty associated with a forecast remains largely unexplored. In this series of studies, we seek to understand if individuals can integrate indirect uncertainty about how “good” a model is (operationalized as a qualitative expression of forecaster confidence) with quantified uncertainty in a prediction (operationalized as a quantile dotplot visualization of a predicted distribution). Our first study results suggest that participants utilize both direct quantitative uncertainty and indirect qualitative uncertainty when conveyed as quantile dotplots and forecaster confidence. In manipulations where forecasters were less sure about their prediction, participants made more conservative judgments. In our second study, we varied the amount of quantified uncertainty (in the form of the SD of the visualized distributions) to examine how participants’ decisions changed under different combinations of quantified uncertainty (variance) and qualitative uncertainty (low, medium, and high forecaster confidence). The second study results suggest that participants updated their judgments in the direction predicted by both qualitative confidence information (e.g., becoming more conservative when the forecaster confidence is low) and quantitative uncertainty (e.g., becoming more conservative when the variance is increased). Based on the findings from both experiments, we recommend that forecasters present qualitative expressions of model confidence whenever possible alongside quantified uncertainty.},
	note = {Accessed: 2022-03-02},
	journal = {Frontiers in Psychology},
	author = {Padilla, Lace M. K. and Powell, Maia and Kay, Matthew and Hullman, Jessica},
	year = {2021},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/PRJPGTCE/Padilla et al. - 2021 - Uncertain About Uncertainty How Qualitative Expre.pdf:application/pdf},
}

@article{amornchat_complex_nodate,
	title = {Complex {Images} for {All} {Learners}},
	language = {en},
	author = {Amornchat, Supada},
	pages = {17},
	file = {Amornchat - Complex Images for All Learners.pdf:/Users/Elavsky/Zotero/storage/VPUBFXMZ/Amornchat - Complex Images for All Learners.pdf:application/pdf},
}

@article{xiong_curse_2020,
	title = {The {Curse} of {Knowledge} in {Visual} {Data} {Communication}},
	volume = {26},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2019.2917689},
	abstract = {A viewer can extract many potential patterns from any set of visualized data values. But that means that two people can see different patterns in the same visualization, potentially leading to miscommunication. Here, we show that when people are primed to see one pattern in the data as visually salient, they believe that naïve viewers will experience the same visual salience. Participants were told one of multiple backstories about political events that affected public polling data, before viewing a graph that depicted those data. One pattern in the data was particularly visually salient to them given the backstory that they heard. They then predicted what naïve viewers would most visually salient on the visualization. They were strongly influenced by their own knowledge, despite explicit instructions to ignore it, predicting that others would find the same patterns to be most visually salient. This result reflects a psychological phenomenon known as the curse of knowledge, where an expert struggles to re-create the state of mind of a novice. The present findings show that the curse of knowledge also plagues the visual perception of data, explaining why people can fail to connect with audiences when they communicate patterns in data.},
	number = {10},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Xiong, Cindy and Van Weelden, Lisanne and Franconeri, Steven},
	month = oct,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Cognition, Cognitive biases, data communication, Data communication, Data mining, Data visualization, Decision making, expertise, information visualization, perception and cognition, Psychology, Visualization},
	pages = {3051--3062},
	file = {Submitted Version:/Users/Elavsky/Zotero/storage/ISGXDS7A/Xiong et al. - 2020 - The Curse of Knowledge in Visual Data Communicatio.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/Elavsky/Zotero/storage/XH38W74Z/8718533.html:text/html},
}

@article{baber_task_1994,
	title = {Task analysis for error identification: a methodology for designing error-tolerant consumer products},
	volume = {37},
	issn = {0014-0139},
	shorttitle = {Task analysis for error identification},
	doi = {10.1080/00140139408964958},
	abstract = {This paper presents an approach to the assessment of ‘intelligent’ consumer products. We assume that a primary reason for the difficulties people encounter in their use of intelligent consumer products is the lack of clear, consistent, and logical task sequences. The approach we have developed aims to illustrate the task sequences required for the purposeful use of ‘intelligent’ products, in order to indicate points at which errors, problems or confusions may occur. The approach combines two well proven methodologies (hierarchical task analysis and state space diagrams) to provide a description of human product interaction.},
	number = {11},
	note = {Accessed: 2022-03-02},
	journal = {Ergonomics},
	author = {BABER, C. and STANTON, N. A.},
	month = nov,
	year = {1994},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00140139408964958},
	keywords = {Hierarchical task analysis, Product assessment and design, State space diagrams},
	pages = {1923--1941},
	file = {Submitted Version:/Users/Elavsky/Zotero/storage/ZIYSZYDK/BABER and STANTON - 1994 - Task analysis for error identification a methodol.pdf:application/pdf;Snapshot:/Users/Elavsky/Zotero/storage/DHC99E5S/00140139408964958.html:text/html},
}

@article{schon_designing_1992,
	series = {Artificial {Intelligence} in {Design} {Conference} 1991 {Special} {Issue}},
	title = {Designing as reflective conversation with the materials of a design situation},
	volume = {5},
	issn = {0950-7051},
	doi = {10.1016/0950-7051(92)90020-G},
	abstract = {The paper considers what it means to capture design knowledge by embodying it in procedures that are expressible in a computer program, distinguishing several possible purposes for such an exercise. Following the lead of David Marr's computational approach to vision, emphasis is placed on ‘phenomenological equivalence’ — that is, first defining the functions of designing, and then specifying how people design. The paper goes on to describe design phenomena that a computational strategy of this kind would have to reproduce. All of them are integral to a view of designing as reflective conversation with the materials of a design situation, and depend on the idea of distinctive design worlds constructed by the designer. These phenomena include: the designer's seeing-moving-seeing, the construction of figures from marks on a page, the appreciation of design qualities, the evolution of design intentions in the course of the design process, the recognition of unintended consequences of move experiments, the storage and deployment of prototypes, which must be placed in transaction with the design situation, and communication across divergent design worlds. Considered as performance criteria for a phenomenologically equivalent computational designer, these phenomena are formidable and threatening. Considered as performance criteria for the construction of a computer-based design assistant, however, they may be highly evocative.},
	language = {en},
	number = {1},
	note = {Accessed: 2022-03-04},
	journal = {Knowledge-Based Systems},
	author = {Schön, D. A.},
	month = mar,
	year = {1992},
	keywords = {computer-based design assistants, design knowledge, design phenomena, designing, phenomenological equivalence},
	pages = {3--14},
	file = {ScienceDirect Snapshot:/Users/Elavsky/Zotero/storage/Y5LW7LXC/095070519290020G.html:text/html},
}

@inproceedings{gaver_what_2012,
	address = {New York, NY, USA},
	series = {{CHI} '12},
	title = {What should we expect from research through design?},
	isbn = {978-1-4503-1015-4},
	doi = {10.1145/2207676.2208538},
	abstract = {In this essay, I explore several facets of research through design in order to contribute to discussions about how the approach should develop. The essay has three parts. In the first, I review two influential theories from the Philosophy of Science to help reflect on the nature of design theory, concluding that research through design is likely to produce theories that are provisional, contingent, and aspirational. In the second part, I discuss three possible interpretations for the diversity of approaches to research through design, and suggest that this variation need not be seen as a sign of inadequate standards or a lack of cumulative progress in the field, but may be natural for a generative endeavour. In the final section, I suggest that, rather than aiming to develop increasingly comprehensive theories of design, practice based research might better view theory as annotation of realised design examples, and particularly portfolios of related pieces. Overall, I suggest that the design research community should be wary of impulses towards convergence and standardisation, and instead take pride in its aptitude for exploring and speculating, particularising and diversifying, and - especially - its ability to manifest the results in the form of new, conceptually rich artefacts.},
	note = {Accessed: 2022-03-03},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Gaver, William},
	month = may,
	year = {2012},
	keywords = {annotation, philosophy of science, portfolios, research through design, theory},
	pages = {937--946},
	file = {Accepted Version:/Users/Elavsky/Zotero/storage/QC4K3DBE/Gaver - 2012 - What should we expect from research through design.pdf:application/pdf},
}

@misc{noauthor_data_nodate,
	title = {Data visualizations {\textbar} {U}.{S}. {Web} {Design} {System} ({USWDS})},
	url = {https://designsystem.digital.gov/components/data-visualizations/},
	abstract = {Data visualizations help communicate patterns and relationships in a data set.},
	author = {{US Government, General Services Administration}},
	language = {en},
	note = {Accessed: 2022-03-04},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/D886LBD5/data-visualizations.html:text/html},
}

@misc{noauthor_improving_nodate,
	title = {Improving {Accessibility} in {Data} {Visualizations} (formerly {USWDS} {Visualization} {Tool}) on 10x},
	url = {https://trello.com/c/OOOres7a/63-improving-accessibility-in-data-visualizations-formerly-uswds-visualization-tool},
	author = {{US Government, 10x}},
	note = {Accessed: 2022-03-04},
	file = {Improving Accessibility in Data Visualizations (formerly USWDS Visualization Tool) on 10x Project Tracking | Trello:/Users/Elavsky/Zotero/storage/4IU7333C/63-improving-accessibility-in-data-visualizations-formerly-uswds-visualization-tool.html:text/html},
}

@misc{noauthor_dataviz_2022,
	title = {Dataviz {Accessibility} {Resources}},
	author = {Gassick, Larene Le and Elavsky, Frank},
	url = {https://github.com/dataviza11y/resources},
	abstract = {A non-exhaustive and in-progress list of people and resources in Accessibility and Data Visualization},
	note = {Accessed: 2022-03-04},
	publisher = {DataViz Accessibility},
	month = mar,
	year = {2022},
	note = {original-date: 2021-01-17T10:34:15Z},
}

@techreport{noauthor_w3c_nodate,
	title = {{W3C} {Accessibility} {Guidelines} ({WCAG}) 3.0},
	type = {{WCAG} {Standard}},
	url = {https://www.w3.org/TR/wcag-3.0/},
	author = {WAI},
	year = {2021},
	institution = {{W3C}},
	note = {Accessed: 2022-03-04},
	file = {W3C Accessibility Guidelines (WCAG) 3.0:/Users/Elavsky/Zotero/storage/X6DSJKZX/wcag-3.0.html:text/html},
}

@inproceedings{irani_turkopticon_13,
    author = {Irani, Lilly C. and Silberman, M. Six},
    title = {Turkopticon: Interrupting Worker Invisibility in Amazon Mechanical Turk},
    year = {2013},
    isbn = {9781450318990},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    doi = {10.1145/2470654.2470742},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    pages = {611–620},
    numpages = {10},
    keywords = {activism, ethics, design, infrastructure, amazon mechanical turk, human computation},
    location = {Paris, France},
    series = {CHI '13}
}

@TechReport{wickham_bin-summarise-smooth_2013,
  author = {Hadley Wickham},
  institution = {had.co.nz},
  title = {Bin-summarise-smooth: a framework for visualising large data},
  year = {2013},
}

@article{scholtz_developing_2011,
	title = {Developing guidelines for assessing visual analytics environments},
	volume = {10},
	issn = {1473-8716},
	doi = {10.1177/1473871611407399},
	abstract = {In this article, we develop guidelines for evaluating visual analytics environments based on a synthesis of reviews for the entries to the 2009 Visual Analytics Science and Technology (VAST) Symposium Challenge and from a user study with professional intelligence analysts. By analyzing the 2009 VAST Challenge reviews, we gained a better understanding of what is important to our reviewers, both visualization researchers and professional analysts. We also report on a small user study with professional analysts to determine the important factors that they use in evaluating visual analysis systems. We also looked at guidelines developed by researchers in various domains and synthesized the results from these three efforts into an initial set for use by others in the community. One challenge for future visual analytics systems is to help in the generation of reports. In our user study, we also worked with analysts to understand the criteria they used to evaluate the quality of analytic reports. We propose that this knowledge will be useful as researchers look at systems to automate some of the report generation.1 From these two efforts, we produced some initial guidelines for evaluating visual analytics environments and for the evaluation of analytic reports. It is important to understand that these guidelines are initial drafts and are limited in scope as the visual analytics systems we evaluated were used in specific tasks. We propose these guidelines as a starting point for the Visual Analytics Community.},
	number = {3},
	note = {Accessed: 2022-03-06},
	journal = {Information Visualization},
	author = {Scholtz, Jean},
	month = jul,
	year = {2011},
	keywords = {guidelines, heuristics, user-centered evaluation, visual analytics},
	pages = {212--231},
}

@article{braun_clarke_thematic_2006,
    author = { Virginia   Braun  and  Victoria   Clarke },
    title = {Using thematic analysis in psychology},
    journal = {Qualitative Research in Psychology},
    volume = {3},
    number = {2},
    pages = {77-101},
    year  = {2006},
    publisher = {Routledge},
    doi = {10.1191/1478088706qp063oa}
}

@conference{oliveira_adapting_2022,
	title = {Adapting {Heuristic} {Evaluation} to {Information} {Visualization} - {A} {Method} for {Defining} a {Heuristic} {Set} by {Heuristic} {Grouping}},
	author = {Oliveira, Maurício and Guimarães da Silva, Celmar},
	isbn = {978-989-758-228-8},
	doi = {10.5220/0006133202250232},
    booktitle={Proceedings of the 12th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - Volume 3: IVAPP, (VISIGRAPP 2017)},
    year={2017},
    pages={225-232},
    publisher={SciTePress},
    organization={INSTICC},
    isbn={978-989-758-228-8},
}

@inproceedings{craft_beyond_2005,
	title = {Beyond guidelines: what can we learn from the visual information seeking mantra?},
	shorttitle = {Beyond guidelines},
	doi = {10.1109/IV.2005.28},
	abstract = {The field of information visualization offers little methodological guidance to practitioners who seek to design novel systems. Though many sources describe the foundations of the domain, few discuss practical methods for solving visualization problems. One frequently cited guideline to design is the "Visual information-seeking mantra", proposed by Shneiderman in 1996. Although often used to inform the design of information visualization systems, it is unclear what use this has been for visualization designers. We reviewed the current literature that references the mantra, noting what authors have found useful about it and why they cite it. The results indicate a need for empirical validation of the mantra and for a method, such as design patterns, to inform a holistic approach to visualisation design.},
	booktitle = {Ninth {International} {Conference} on {Information} {Visualisation} ({IV}'05)},
	author = {Craft, B. and Cairns, P.},
	month = jul,
	year = {2005},
	note = {ISSN: 2375-0138},
	keywords = {Art, Data visualization, Design methodology, Educational institutions, Filters, Guidelines, Information analysis, Patterns, Software design, Taxonomy, Visual Information Seeking Mantra, Visualization Methodology},
	pages = {110--118},
	file = {IEEE Xplore Abstract Record:/Users/Elavsky/Zotero/storage/QQJM6PQH/1509067.html:text/html},
}

@inproceedings{forsell_heuristic_2010,
	address = {New York, NY, USA},
	series = {{AVI} '10},
	title = {An heuristic set for evaluation in information visualization},
	isbn = {978-1-4503-0076-6},
	doi = {10.1145/1842993.1843029},
	abstract = {Evaluation is a key research challenge within the international Information Visualization (InfoVis) community, and Heuristic Evaluation is one recognized method. Various sets of heuristics have been proposed but there remains no consensus as to which heuristics are most useful for addressing aspects specific to the complex interactive visual displays used in modern InfoVis systems. This paper presents a first effort to empirically determine a new set of such general heuristics tailored for Heuristic Evaluation of common and important usability problems in InfoVis techniques. Participants in the study rated how well a total of 63 heuristics from 6 earlier published heuristic sets could explain a collection of 74 usability problems derived from earlier InfoVis evaluations. The results were used to synthesize 10 heuristics that, as a set, provided the highest explanatory coverage. The paper also stresses the challenges for future research to validate and further improve upon this set.},
	note = {Accessed: 2022-03-06},
	booktitle = {Proceedings of the {International} {Conference} on {Advanced} {Visual} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Forsell, Camilla and Johansson, Jimmy},
	month = may,
	year = {2010},
	keywords = {heuristic evaluation, heuristics, information visualization},
	pages = {199--206},
}

@article{ladner_design_2015,
	title = {Design for user empowerment},
	volume = {22},
	issn = {1072-5520},
	doi = {10.1145/2723869},
	number = {2},
	note = {Accessed: 2022-03-07},
	journal = {Interactions},
	author = {Ladner, Richard E.},
	month = feb,
	year = {2015},
	pages = {24--29},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/Z72ATFSM/Ladner - 2015 - Design for user empowerment.pdf:application/pdf},
}

@inproceedings{gray_reprioritizing_2014,
	address = {New York, NY, USA},
	series = {{DIS} '14},
	title = {Reprioritizing the relationship between {HCI} research and practice: bubble-up and trickle-down effects},
	isbn = {978-1-4503-2902-6},
	shorttitle = {Reprioritizing the relationship between {HCI} research and practice},
	doi = {10.1145/2598510.2598595},
	booktitle = {Proceedings of the 2014 conference on {Designing} interactive systems},
	publisher = {Association for Computing Machinery},
	author = {Gray, Colin M. and Stolterman, Erik and Siegel, Martin A.},
	month = jun,
	year = {2014},
	keywords = {community of practice, design methods, design practice, interaction design},
	pages = {725--734},
	file = {Full Text PDF:/Users/Elavsky/Zotero/storage/QTHDBK7V/Gray et al. - 2014 - Reprioritizing the relationship between HCI resear.pdf:application/pdf},
}

@misc{noauthor_everything_nodate,
	title = {Everything {You} {Never} {Wanted} to {Know} {About} {Google} {Maps}' {Parameters}},
	url = {https://moz.com/blog/everything-you-never-wanted-to-know-about-google-maps-parameters},
	author = {Watson-Wailes, Pete},
	language = {en-US},
	note = {Accessed: 2022-03-10},
	journal = {Moz},
	file = {Snapshot:/Users/Elavsky/Zotero/storage/QFVVWNJP/everything-you-never-wanted-to-know-about-google-maps-parameters.html:text/html},
}

@misc{noauthor_complex_nodate,
	title = {Complex {Images} • {Images} • {WAI} {Web} {Accessibility} {Tutorials}},
	url = {https://www.w3.org/WAI/tutorials/images/complex/},
	author = {WAI},
	note = {Accessed: 2022-03-10},
	file = {Complex Images • Images • WAI Web Accessibility Tutorials:/Users/Elavsky/Zotero/storage/PG6C8FKF/complex.html:text/html},
}
